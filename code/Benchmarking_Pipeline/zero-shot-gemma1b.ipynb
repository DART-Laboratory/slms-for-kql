{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1270e82a-9abf-426e-95e0-64ffa1d09883",
   "metadata": {},
   "source": [
    "### Zero-Shot Notebook (Benchmarking Pipeline):\n",
    "\n",
    "The following notebook is used to answer Research Question #1 (RQ1) in the SLM Threat Query Analysis Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c853f7c-06f3-4dc7-b913-7ed0a7f19b6c",
   "metadata": {},
   "source": [
    "### Step #1: Import Packages:\n",
    "\n",
    "Import all the packages that are needed to run the notebook. To install packages, you will need to run the following command in either your Terminal or in any of the cells:\n",
    "\n",
    "```!pip install -r ../requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99994c1e-2c0f-4767-8c44-253c1fd4f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "import pandas as pd\n",
    "from kql_benchmarking_pipeline import KQLBenchmarkPipeline\n",
    "\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import tiktoken\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from ZeroShot import *\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7045d0-4240-40b0-b828-335782d94a95",
   "metadata": {},
   "source": [
    "### Step #2: Import Evaluation Dataset:\n",
    "\n",
    "Read the Evaluation Dataset into a DataFrame, and create a new DataFrame to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27fa19-3f4e-4c06-aee9-36f93061730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "\n",
    "eval_df = pd.read_json(path_or_buf='../NL2KQL_Remakes/data/evaluation/Defender_Evaluation.jsonl', lines=True)\n",
    "df = pd.DataFrame(columns = ['NLQ', 'KQL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea087f8-1e12-439a-a9ea-52e93e54e00f",
   "metadata": {},
   "source": [
    "### Step #3: Specify the Model:\n",
    "\n",
    "Next you will need to specify which type of model that you will use. There's multiple options that you can specify, below are some examples:\n",
    "\n",
    "- HuggingFace-Based Models: \n",
    "    - \"google/gemma-3-1b-it\" (Google's Gemma-3-1B-IT model)\n",
    "    - \"google/gemma-3-4b-it\" (Google's Gemma-3-4B-IT model)\n",
    "    - \"microsoft/phi-4\" (Microsoft's Phi-4 model)\n",
    "    - \"microsoft/phi-4-mini-instruct\" (Microsoft's Phi-4-Mini-Instruct model)\n",
    "\n",
    "- GenAI-Based Models:\n",
    "    - \"gemini-2.0-flash\" (Google's Gemini 2.0 Flash model)\n",
    "\n",
    "- OpenAI-Based Models:\n",
    "    - \"gpt-4o\" (OpenAI's GPT-4o model)\n",
    "    - \"gpt-5\" (OpenAI's GPT-5 model)\n",
    "\n",
    "You can add additional models that are not present in these lists, but you must ensure that they are in the correct specified format. For example, if you plan to load a model from Huggingface, you must set your ```mode``` variable to ```huggingface```. If you wish to load a model from OpenAI, you must set the ```mode``` variable to ```openai```, and if you wish to load a model using the Google GenAI client, you must specify the ```mode``` variable to ```genai-client```. In addition, fill in the variable below with the proper model to use. \n",
    "\n",
    "If you plan to use a different model, **YOU MAY NEED TO ADD FILTERING LOGIC TO ```ZeroShot.py```** file that is also present in the ```Benchmarking_Pipeline``` folder. Although most of the code is generalizable across multiple models, sidecases always do exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc2d1a-f0ee-416b-9150-d21121739e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Three possible options for mode: huggingface, openai (only for GPT-models), genai-client (only for Gemini-based models):\n",
    "mode = \"huggingface\"\n",
    "\n",
    "# Specify which folder to store the .yaml files:\n",
    "results_folder = \"\"\n",
    "os.makedirs(results_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7008c10-da77-4553-b0bb-9137da3c4aaa",
   "metadata": {},
   "source": [
    "### Step #4: Run the Pipeline:\n",
    "\n",
    "The following cell runs the entire pipeline and collects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c82502-e362-41d0-83c8-0e8ba6a52468",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "latencies = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "\n",
    "# If it is a GPT Model, only run the results ONCE:\n",
    "if mode == \"openai\":\n",
    "\n",
    "    for i in range(0, 1):\n",
    "        client = ZeroShot(model_name, mode, eval_df)\n",
    "        pipeline = KQLBenchmarkPipeline(client)\n",
    "        pipeline.run()\n",
    "        pipeline.save_results(f\"{results_folder}/{model_name}-zero-shot-{i}.yaml\")\n",
    "        latencies.append((sum(client.times))/(len(client.times)))\n",
    "\n",
    "        revised_name = model_name\n",
    "\n",
    "# All other models - run five times:\n",
    "else:\n",
    "    for i in range(0, 5):\n",
    "        client = ZeroShot(model_name, mode, eval_df)\n",
    "        pipeline = KQLBenchmarkPipeline(client)\n",
    "        pipeline.run()\n",
    "        \n",
    "        if mode == 'huggingface':\n",
    "            revised_name = re.search(r'\\/(.*)', model_name, flags=re.DOTALL).group(1)\n",
    "        else:\n",
    "            revised_name = model_name\n",
    "            \n",
    "        pipeline.save_results(f\"{results_folder}/{revised_name}-zero-shot-{i}.yaml\")\n",
    "        latencies.append((sum(client.times))/(len(client.times)))\n",
    "\n",
    "        # Take the input/output tokens, and move the model to cpu:\n",
    "        if mode == 'huggingface':\n",
    "            input_tokens.append(client.input_tokens)\n",
    "            output_tokens_local = 0\n",
    "            for idx, row in client.df.iterrows():\n",
    "                output_tokens_local = output_tokens_local + len(client.tokenizer(row['Full Response'][0]['generated_text'])[\"input_ids\"])\n",
    "            \n",
    "            output_tokens.append(output_tokens_local)\n",
    "            client.model = client.model.to(\"cpu\")\n",
    "\n",
    "        if mode == 'genai-client':\n",
    "            genai_client = genai.Client(api_key = token_config['genai']['token'], http_options=HttpOptions(timeout=3*60*1000))\n",
    "            \n",
    "            input_tokens_local = 0\n",
    "            output_tokens_local = 0\n",
    "            count = 0\n",
    "            \n",
    "            for idx, row in client.df.iterrows():\n",
    "                input_tokens_local += genai_client.models.count_tokens(model=\"gemini-2.0-flash\", contents = f\"You are a programmer using the Kusto Query Language with Microsoft Defender. Generate a KQL query that answers the following request. Return only the KQL code without any explanation. {eval_df.loc[count]['context']}\").total_tokens\n",
    "                output_tokens_local += genai_client.models.count_tokens(model=\"gemini-2.0-flash\", contents = str(row['Full Response'])).total_tokens\n",
    "                count = count + 1\n",
    "\n",
    "            input_tokens.append(input_tokens_local)\n",
    "            output_tokens.append(output_tokens_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ac1df-afb8-4d78-bdcc-e42ddd3bd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = len(client.df)           # or client.df.shape[0]\n",
    "print(\"Processed queries:\", num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7485fcc-dd49-43b8-84ef-ff9df9111f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(latencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5550079-19c0-45b4-8b8f-d55c09a0b08b",
   "metadata": {},
   "source": [
    "### Step #5: Merge Results together\n",
    "\n",
    "The following saves the results into a yaml file. Please note that while we have code that has done extensive regex cleaning, manual revision is still required in order to check for any extraneous characters that might affect the metrics scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbd89f-5eac-453b-8837-24a3a6f0ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, yaml\n",
    "\n",
    "# Merge any shards you have (adjust the glob if needed)\n",
    "paths = sorted(glob.glob(f\"{results_folder}/{revised_name}-zero-shot-*.yaml\"))\n",
    "\n",
    "merged = {\"queries\": []}\n",
    "for p in paths:\n",
    "    with open(p, \"r\") as f:\n",
    "        y = yaml.safe_load(f) or {}\n",
    "        merged[\"queries\"].extend(y.get(\"queries\", []))\n",
    "\n",
    "with open(f\"{results_folder}/{revised_name}-zero-shot.yaml\", \"w\") as f:\n",
    "    yaml.dump(merged, f, sort_keys=False, default_style='|', allow_unicode=True, width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77550a28-6114-4fb5-b86a-d349ca1f77b4",
   "metadata": {},
   "source": [
    "### Step #6: Latency Output\n",
    "\n",
    "Prints out the average latency for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba8440-937b-4acd-a884-16bf818a0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "print(f\"Avg. Latency: {round((sum(latencies))/(len(latencies)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f197527-ab64-464b-a57f-321945ebf59e",
   "metadata": {},
   "source": [
    "### Step #7: Cost Analysis:\n",
    "\n",
    "Prints out the average cost for running all queries. We calculate the costs in different ways:\n",
    "1. ##### HuggingFace Models:\n",
    "   - We calculate the number of input and output tokens as the queries are run. Once we calculate the total number of input and output tokens, we save them into list variables and can calculate the total costs from there.\n",
    "  \n",
    "2. ##### OpenAI Models:\n",
    "   - We use the ```tiktoken``` package to calculate the OpenAI model input and output tokens.\n",
    "  \n",
    "3. ##### GenAI-Client Models:\n",
    "    - Due to potential risks of crashing, we calculate tokens AFTER all results have been obtained.\n",
    "  \n",
    "In ALL cases, you must update the input token costs and output token costs (per million tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b115c-4ac6-494e-88e9-6938783a81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: YOU MUST CHANGE THESE VALUES BASED ON MODEL PRICING:\n",
    "llm_input_cost_per_million = 0\n",
    "llm_output_cost_per_million = 0\n",
    "cost = 0\n",
    "\n",
    "if mode == 'openai':\n",
    "\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    count = 0\n",
    "    \n",
    "    encoding = tiktoken.encoding_for_model(revised_name)\n",
    "\n",
    "    # Change the 230 to an actual len number:\n",
    "    for i in range(0,10):\n",
    "        input_tokens = input_tokens + len(encoding.encode(f\"You are a programmer using the Kusto Query Language with Microsoft Defender. Generate a KQL query that answers the following request.  Return only the KQL code without any explanation. {eval_df.loc[count]['context']}\"))\n",
    "        count = count + 1\n",
    "\n",
    "    for entry in client.df['Full Response']:\n",
    "        output_tokens = output_tokens + len(encoding.encode(entry))\n",
    "    \n",
    "    cost = ((llm_input_cost_per_million * input_tokens)/1000000) + ((llm_output_cost_per_million * output_tokens)/1000000)\n",
    "    avg_cost = cost/1\n",
    "    \n",
    "    print(f\"Average Total Cost: ${round(avg_cost, 3)}\")\n",
    "\n",
    "elif mode == 'huggingface' or mode == 'genai-client':\n",
    "\n",
    "    for entry in list(zip(input_tokens, output_tokens)):\n",
    "        cost += ((llm_input_cost_per_million * entry[0])/1000000) + ((llm_output_cost_per_million * entry[1])/1000000)\n",
    "\n",
    "    avg_cost = cost/5\n",
    "    print(f\"Average Total Cost: ${round(avg_cost, 3)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c337d5-4661-4a1e-9159-e6d41c432d7a",
   "metadata": {},
   "source": [
    "### Step #8: Get Metrics:\n",
    "\n",
    "The following snippets will obtain all metrics for the results that you have just calculated. Please note that full path **must** be specified for ```file_of_interest``` and ```folder```. If you need help specifying the full path, use the ```!pwd``` command in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0eb4f-c95f-4486-9e8b-7237aa5a567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../offline_metrics_pipeline/offline-metrics-pipeline/offline_metrics_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "folder = \"\"\n",
    "results_file = \"testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e93cc1-95b0-4d5f-b145-c60c734274ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python {runner} {file_of_interest} {folder} {results_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
