{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1270e82a-9abf-426e-95e0-64ffa1d09883",
   "metadata": {},
   "source": [
    "### Zero-Shot Notebook (Benchmarking Pipeline):\n",
    "\n",
    "The following notebook is used to answer Research Question #1 (RQ1) in the SLM Threat Query Analysis Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c853f7c-06f3-4dc7-b913-7ed0a7f19b6c",
   "metadata": {},
   "source": [
    "### Step #1: Import Packages:\n",
    "\n",
    "Import all the packages that are needed to run the notebook. To install packages, you will need to run the following command in either your Terminal or in any of the cells:\n",
    "\n",
    "```!pip install -r ../requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99994c1e-2c0f-4767-8c44-253c1fd4f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "import pandas as pd\n",
    "from kql_benchmarking_pipeline import KQLBenchmarkPipeline\n",
    "\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import tiktoken\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from ZeroShot import *\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7045d0-4240-40b0-b828-335782d94a95",
   "metadata": {},
   "source": [
    "### Step #2: Import Evaluation Dataset:\n",
    "\n",
    "Read the Evaluation Dataset into a DataFrame, and create a new DataFrame to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27fa19-3f4e-4c06-aee9-36f93061730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "\n",
    "eval_df = pd.read_json(path_or_buf='../NL2KQL_Remakes/data/evaluation/Defender_Evaluation.jsonl', lines=True)\n",
    "df = pd.DataFrame(columns = ['NLQ', 'KQL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea087f8-1e12-439a-a9ea-52e93e54e00f",
   "metadata": {},
   "source": [
    "### Step #3: Specify the Model:\n",
    "\n",
    "Next you will need to specify which type of model that you will use. There's multiple options that you can specify, below are some examples:\n",
    "\n",
    "- HuggingFace-Based Models: \n",
    "    - \"google/gemma-3-1b-it\" (Google's Gemma-3-1B-IT model)\n",
    "    - \"google/gemma-3-4b-it\" (Google's Gemma-3-4B-IT model)\n",
    "    - \"microsoft/phi-4\" (Microsoft's Phi-4 model)\n",
    "    - \"microsoft/phi-4-mini-instruct\" (Microsoft's Phi-4-Mini-Instruct model)\n",
    "\n",
    "- GenAI-Based Models:\n",
    "    - \"gemini-2.0-flash\" (Google's Gemini 2.0 Flash model)\n",
    "\n",
    "- OpenAI-Based Models:\n",
    "    - \"gpt-4o\" (OpenAI's GPT-4o model)\n",
    "    - \"gpt-5\" (OpenAI's GPT-5 model)\n",
    "\n",
    "You can add additional models that are not present in these lists, but you must ensure that they are in the correct specified format. For example, if you plan to load a model from Huggingface, you must set your ```mode``` variable to ```huggingface```. If you wish to load a model from OpenAI, you must set the ```mode``` variable to ```openai```, and if you wish to load a model using the Google GenAI client, you must specify the ```mode``` variable to ```genai-client```. In addition, fill in the variable below with the proper model to use. \n",
    "\n",
    "If you plan to use a different model, **YOU MAY NEED TO ADD FILTERING LOGIC TO ```ZeroShot.py```** file that is also present in the ```Benchmarking_Pipeline``` folder. Although most of the code is generalizable across multiple models, sidecases always do exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc2d1a-f0ee-416b-9150-d21121739e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt-5\"\n",
    "\n",
    "# Three possible options for mode: huggingface, openai (only for GPT-models), genai-client (only for Gemini-based models):\n",
    "mode = \"openai\"\n",
    "\n",
    "# Specify which folder to store the .yaml files:\n",
    "results_folder = \"\"\n",
    "os.makedirs(results_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7008c10-da77-4553-b0bb-9137da3c4aaa",
   "metadata": {},
   "source": [
    "### Step #4: Run the Pipeline:\n",
    "\n",
    "The following cell runs the entire pipeline and collects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c82502-e362-41d0-83c8-0e8ba6a52468",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "latencies = []\n",
    "input_tokens = []\n",
    "output_tokens = []\n",
    "\n",
    "# If it is a GPT Model, only run the results ONCE:\n",
    "if mode == \"openai\":\n",
    "\n",
    "    for i in range(0, 1):\n",
    "        client = ZeroShot(model_name, mode, eval_df)\n",
    "        pipeline = KQLBenchmarkPipeline(client)\n",
    "        pipeline.run()\n",
    "        pipeline.save_results(f\"{results_folder}/{model_name}-zero-shot-{i}.yaml\")\n",
    "        latencies.append((sum(client.times))/(len(client.times)))\n",
    "\n",
    "        revised_name = model_name\n",
    "\n",
    "# All other models - run five times:\n",
    "else:\n",
    "    for i in range(0, 5):\n",
    "        client = ZeroShot(model_name, mode, eval_df)\n",
    "        pipeline = KQLBenchmarkPipeline(client)\n",
    "        pipeline.run()\n",
    "        \n",
    "        if mode == 'huggingface':\n",
    "            revised_name = re.search(r'\\/(.*)', model_name, flags=re.DOTALL).group(1)\n",
    "        else:\n",
    "            revised_name = model_name\n",
    "            \n",
    "        pipeline.save_results(f\"{results_folder}/{revised_name}-zero-shot-{i}.yaml\")\n",
    "        latencies.append((sum(client.times))/(len(client.times)))\n",
    "\n",
    "        # Take the input/output tokens, and move the model to cpu:\n",
    "        if mode == 'huggingface':\n",
    "            input_tokens.append(client.input_tokens)\n",
    "            output_tokens_local = 0\n",
    "            for idx, row in client.df.iterrows():\n",
    "                output_tokens_local = output_tokens_local + len(client.tokenizer(row['Full Response'][0]['generated_text'])[\"input_ids\"])\n",
    "            \n",
    "            output_tokens.append(output_tokens_local)\n",
    "            client.model = client.model.to(\"cpu\")\n",
    "\n",
    "        if mode == 'genai-client':\n",
    "            genai_client = genai.Client(api_key = token_config['genai']['token'], http_options=HttpOptions(timeout=3*60*1000))\n",
    "            \n",
    "            input_tokens_local = 0\n",
    "            output_tokens_local = 0\n",
    "            count = 0\n",
    "            \n",
    "            for idx, row in client.df.iterrows():\n",
    "                input_tokens_local += genai_client.models.count_tokens(model=\"gemini-2.0-flash\", contents = f\"You are a programmer using the Kusto Query Language with Microsoft Defender. Generate a KQL query that answers the following request. Return only the KQL code without any explanation. {eval_df.loc[count]['context']}\").total_tokens\n",
    "                output_tokens_local += genai_client.models.count_tokens(model=\"gemini-2.0-flash\", contents = str(row['Full Response'])).total_tokens\n",
    "                count = count + 1\n",
    "\n",
    "            input_tokens.append(input_tokens_local)\n",
    "            output_tokens.append(output_tokens_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ac1df-afb8-4d78-bdcc-e42ddd3bd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = len(client.df)           # or client.df.shape[0]\n",
    "print(\"Processed queries:\", num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7485fcc-dd49-43b8-84ef-ff9df9111f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(latencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af95d88-9d18-4fbe-94fd-e539bb67658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(client.times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5550079-19c0-45b4-8b8f-d55c09a0b08b",
   "metadata": {},
   "source": [
    "### Step #5: Merge Results together\n",
    "\n",
    "The following saves the results into a yaml file. Please note that while we have code that has done extensive regex cleaning, manual revision is still required in order to check for any extraneous characters that might affect the metrics scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbd89f-5eac-453b-8837-24a3a6f0ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, yaml\n",
    "\n",
    "# Merge any shards you have (adjust the glob if needed)\n",
    "paths = sorted(glob.glob(f\"{results_folder}/{model_name}-zero-shot-*.yaml\"))\n",
    "\n",
    "merged = {\"queries\": []}\n",
    "for p in paths:\n",
    "    with open(p, \"r\") as f:\n",
    "        y = yaml.safe_load(f) or {}\n",
    "        merged[\"queries\"].extend(y.get(\"queries\", []))\n",
    "\n",
    "with open(f\"{results_folder}/{model_name}-zero-shot.yaml\", \"w\") as f:\n",
    "    yaml.dump(merged, f, sort_keys=False, default_style='|', allow_unicode=True, width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77550a28-6114-4fb5-b86a-d349ca1f77b4",
   "metadata": {},
   "source": [
    "### Step #6: Latency Output\n",
    "\n",
    "Prints out the average latency for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba8440-937b-4acd-a884-16bf818a0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell:\n",
    "print(f\"Avg. Latency: {round((sum(client.times))/(len(client.times)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f197527-ab64-464b-a57f-321945ebf59e",
   "metadata": {},
   "source": [
    "### Step #7: Cost Analysis:\n",
    "\n",
    "Prints out the average cost for running all queries. We calculate the costs in different ways:\n",
    "1. ##### HuggingFace Models:\n",
    "   - We calculate the number of input and output tokens as the queries are run. Once we calculate the total number of input and output tokens, we save them into list variables and can calculate the total costs from there.\n",
    "  \n",
    "2. ##### OpenAI Models:\n",
    "   - We use the ```tiktoken``` package to calculate the OpenAI model input and output tokens.\n",
    "  \n",
    "3. ##### GenAI-Client Models:\n",
    "    - Due to potential risks of crashing, we calculate tokens AFTER all results have been obtained.\n",
    "  \n",
    "In ALL cases, you must update the input token costs and output token costs (per million tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b115c-4ac6-494e-88e9-6938783a81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "# === PRICING (USD per 1M tokens) ===\n",
    "llm_input_cost_per_million  = 1.250\n",
    "llm_output_cost_per_million = 10.000\n",
    "\n",
    "# ===== helper: robust encoder getter =====\n",
    "def _encoding_name_for(model_name: str) -> str:\n",
    "    m = (model_name or \"\").lower()\n",
    "    if any(x in m for x in [\"gpt-4o\", \"gpt-4.1\", \"4o-mini\", \"o200k\", \"gpt-5\"]):\n",
    "        return \"o200k_base\"\n",
    "    if any(x in m for x in [\"gpt-4\", \"gpt-3.5\", \"cl100k\", \"text-embedding-3\"]):\n",
    "        return \"cl100k_base\"\n",
    "    if any(x in m for x in [\"text-davinci-003\", \"code-davinci\", \"p50k\"]):\n",
    "        return \"p50k_base\"\n",
    "    return \"cl100k_base\"\n",
    "\n",
    "def get_tiktoken_encoder(model_name: str = None):\n",
    "    \"\"\"Return a tiktoken encoding object (not a string).\"\"\"\n",
    "    try:\n",
    "        if model_name:\n",
    "            return tiktoken.encoding_for_model(model_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: return encoding by name\n",
    "    return tiktoken.get_encoding(_encoding_name_for(model_name or \"\"))\n",
    "\n",
    "# ====== Safe cost calculation ======\n",
    "if mode == \"openai\":\n",
    "    # choose model name you used for the API call; use None if unknown\n",
    "    model_used = \"gpt-5\"\n",
    "    enc = get_tiktoken_encoder(model_used)\n",
    "\n",
    "    # prepare prompt template (same as you used)\n",
    "    prompt_tmpl = (\n",
    "        \"You are a programmer using the Kusto Query Language with Microsoft Defender. \"\n",
    "        \"Generate a KQL query that answers the following request. \"\n",
    "        \"Return only the KQL code without any explanation. {context}\"\n",
    "    )\n",
    "\n",
    "    # How many examples to count (do not exceed eval_df length)\n",
    "    max_examples = 10\n",
    "    n_examples = min(max_examples, len(eval_df))\n",
    "\n",
    "    input_tokens = 0\n",
    "    for i in range(n_examples):\n",
    "        ctx = \"\"\n",
    "        # safe access: .iloc handles integer position\n",
    "        try:\n",
    "            ctx = eval_df.iloc[i].get(\"context\", \"\") if isinstance(eval_df.iloc[i], dict) else eval_df.iloc[i].get(\"context\", \"\") \n",
    "        except Exception:\n",
    "            # fallback: use .loc with index if eval_df is normal DataFrame\n",
    "            try:\n",
    "                ctx = eval_df.loc[eval_df.index[i], \"context\"]\n",
    "            except Exception:\n",
    "                ctx = \"\"\n",
    "        if pd.isna(ctx):\n",
    "            ctx = \"\"\n",
    "        prompt = prompt_tmpl.format(context=str(ctx))\n",
    "        input_tokens += len(enc.encode(prompt))\n",
    "\n",
    "    # Count output tokens from client.df['Full Response']\n",
    "    output_tokens = 0\n",
    "    if hasattr(client, \"df\") and \"Full Response\" in client.df.columns:\n",
    "        for resp in client.df[\"Full Response\"]:\n",
    "            if isinstance(resp, str) and len(resp):\n",
    "                output_tokens += len(enc.encode(resp))\n",
    "    else:\n",
    "        # fallback: try to find a response-like column\n",
    "        found = False\n",
    "        if hasattr(client, \"df\"):\n",
    "            for c in client.df.columns:\n",
    "                if \"resp\" in c.lower() or \"full\" in c.lower() or \"response\" in c.lower():\n",
    "                    for resp in client.df[c]:\n",
    "                        if isinstance(resp, str) and len(resp):\n",
    "                            output_tokens += len(enc.encode(resp))\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            print(\"Warning: couldn't find client.df['Full Response'] or alternative; output_tokens set to 0\")\n",
    "\n",
    "    cost = (llm_input_cost_per_million * input_tokens) / 1_000_000.0 \\\n",
    "         + (llm_output_cost_per_million * output_tokens) / 1_000_000.0\n",
    "\n",
    "    print(f\"Input tokens:  {input_tokens:,}\")\n",
    "    print(f\"Output tokens: {output_tokens:,}\")\n",
    "    print(f\"Average Total Cost: ${cost:.3f}\")\n",
    "\n",
    "elif mode in (\"huggingface\", \"genai-client\"):\n",
    "    # Expect you have per-prompt lists of tokens (input_tokens_list, output_tokens_list)\n",
    "    # If you only have aggregated numbers, adapt accordingly.\n",
    "    # Example: if you kept lists:\n",
    "    try:\n",
    "        input_list = input_tokens  # in your original code this might be a list\n",
    "        output_list = output_tokens\n",
    "        if not isinstance(input_list, list) or not isinstance(output_list, list):\n",
    "            raise TypeError(\"expected input_tokens and output_tokens to be lists of per-prompt counts\")\n",
    "        if len(input_list) != len(output_list):\n",
    "            print(\"Warning: input/output lists differ in length; pairing by min length\")\n",
    "        pairs = list(zip(input_list, output_list))\n",
    "    except Exception:\n",
    "        # If you only have totals (int), wrap them so the code still runs\n",
    "        pairs = [(int(input_tokens), int(output_tokens))]\n",
    "\n",
    "    total_cost = 0.0\n",
    "    for itoks, otoks in pairs:\n",
    "        total_cost += (llm_input_cost_per_million * int(itoks)) / 1_000_000.0 \\\n",
    "                    + (llm_output_cost_per_million * int(otoks)) / 1_000_000.0\n",
    "\n",
    "    runs = 5  # adjust to the number of runs you averaged over\n",
    "    avg_cost = total_cost / max(1, runs)\n",
    "    print(f\"Average Total Cost over {runs} runs: ${avg_cost:.3f}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c337d5-4661-4a1e-9159-e6d41c432d7a",
   "metadata": {},
   "source": [
    "### Step #8: Get Metrics:\n",
    "\n",
    "The following snippets will obtain all metrics for the results that you have just calculated. Please note that full path **must** be specified for ```file_of_interest``` and ```folder```. If you need help specifying the full path, use the ```!pwd``` command in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0eb4f-c95f-4486-9e8b-7237aa5a567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../offline_metrics_pipeline/offline-metrics-pipeline/offline_metrics_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "folder = \"\"\n",
    "results_file = \"testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e93cc1-95b0-4d5f-b145-c60c734274ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python {runner} {file_of_interest} {folder} {results_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
