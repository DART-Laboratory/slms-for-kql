{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9474ca2-a4ce-4a2c-8712-3531c608311f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NL2KQL (Generalized):\n",
    "\n",
    "The following notebook is a replication of [NL2KQL](https://arxiv.org/pdf/2404.02933). There are multiple main components involved in this pipeline:\n",
    "\n",
    "- Schema Refiner\n",
    "- Few-Shot Selector\n",
    "- Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8353d0-a9e8-4cd7-a40a-204db445c165",
   "metadata": {},
   "source": [
    "### Step #1: Import Necessary Packages:\n",
    "\n",
    "Import all the packages that are needed to run the notebook. To install packages, you will need to run the following command in either your Terminal or in any of the cells:\n",
    "\n",
    "```!pip install -r ../requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0371-371c-4758-b990-dd3853576ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Necessary Packages:\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning-based packages:\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from helpers import *\n",
    "\n",
    "import pathlib\n",
    "import textwrap\n",
    "import time\n",
    "import pandas\n",
    "import json\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google import genai as genai_client\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b08df-4835-497a-be39-524b0a9d8e40",
   "metadata": {},
   "source": [
    "Below you will need to specify the mode and model that you wish to test. You may choose from any of the following modes:\n",
    "\n",
    "```mode```:\n",
    "- genai-client\n",
    "- openai\n",
    "\n",
    "Please note that you will need to specify the model in the correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bd4b1-c389-4092-9340-93449fb6eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the API and model that you wish to test:\n",
    "mode = \"genai-client\"\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "client = genai_client.Client(api_key=token_config['genai']['token'])\n",
    "\n",
    "if mode == \"genai-client\":\n",
    "    client_two = genai_client.Client(api_key=token_config['genai']['token'])\n",
    "elif mode == \"openai\":\n",
    "    client_two = OpenAI(api_key = token_config['openai']['token'])\n",
    "\n",
    "# Path to prompt template:\n",
    "prompt_template_path = \"prompt_template.txt\"\n",
    "\n",
    "# Should be a boolean (True/False):\n",
    "value_placeholder = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e5feb-267d-492a-8435-66611b722401",
   "metadata": {},
   "source": [
    "### Step #2: Creating the Few-Shot Embedding Store Database\n",
    "\n",
    "The following box creates a Few-Shot Embedding Store Database. The purpose of this database is to create a variety of KQL examples that can be provided to the LLM in order to improve KQL code generation accuracy. Note that if an FSDB database has already been created, you do not need to run the block below (running the block below when an FSDB database exists will result in a message that says \"FSDB already exists for Defender\"). Instead run the second block below, which reads in the Defender FSDB database. The first block is **commented**, if you need to make changes to the FSDB then uncomment and run the cell.\n",
    "\n",
    "If you need to make any changes to how you generate the FSDB, see the ```helpers.py``` file.\n",
    "\n",
    "NOTE: For the purposes of this project, we focus exclusively on Microsoft Defender for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96e726-208d-491d-ba3b-fc4a435dbefd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# themes = [\n",
    "#     \"Explore: Look for signs or hints of a security attack\",\n",
    "#     \"Expansion: Searches for additional contextual understanding\",\n",
    "#     \"Detect: Look for events related to a security attack\",\n",
    "#     \"Remediate: Identify events for a given entity or asset\",\n",
    "#     \"Report: Provide summary statistics for reporting\"\n",
    "# ]\n",
    "\n",
    "# schema_file = \"defender_fsdb_new.json\"\n",
    "# fsdb = generate_fsdb(themes, schema_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fd3e8-637b-46f0-a699-9181bd456981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fsdb/defender_fsdb.json', 'r') as f:\n",
    "    fsdb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fda19a-db08-4496-951d-6e4d7e63eddd",
   "metadata": {},
   "source": [
    "### Step #3: Generating the Table Embedding Store (Semantic Data Catalog)\n",
    "\n",
    "As part of the Semantic Data Catalog, there are two different types of embeddings: Table Embeddings and Value Embeddings. In the next few steps, we will first make generate a table embedding dictionary and later build a value embedding dictionary to simulate the table embedding and value embedding stores discussed in NL2KQL:\n",
    "\n",
    "Note: If a Table Embeddings file has already been created, then run the third block below instead to load the Table Embeddings directly from a json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef04fe3-164c-45bc-804b-8ed1152b8403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/miscellaneous/defender.yml', 'r') as file:\n",
    "    defender_information = yaml.safe_load(file)\n",
    "    \n",
    "defender_embeddings = generate_table_embeddings(defender_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615707e8-51b0-4440-9c51-adf7972f29b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('table_embeddings.json', 'w') as f:\n",
    "    json.dump(defender_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4b9e9-ed66-4803-90e3-e5eb559cf459",
   "metadata": {},
   "source": [
    "If you have already run the two code block snippets above before, then run the following block instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776b6a2-2787-4a3f-a8d1-83ea27283d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL:\n",
    "\n",
    "with open('data/embeddings/defender_table_embeddings.json', 'r') as f:\n",
    "    table_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d45f1-4f7d-4655-9761-3a815b8ccd23",
   "metadata": {},
   "source": [
    "### Step #4: Generating the Value Embedding Stores (Semantic Data Catalog)\n",
    "\n",
    "To preserve efficiency, we then find the embeddings of the columns of the filtered tables. Normally it would be efficient to store them all at once, but for this replication we just aim to generate the embeddings once we have the desired tables. We are bound by requests per day from the queries (1,500 per day) when querying Google Gemini 2.0, which is why we find the tables first, and then their respective embeddings.\n",
    "\n",
    "Because the process takes a bit of time to generate, we do not supply the code here used to generate the embeddings (that is included in a separate notebook). Instead, just load the value embeddings through the block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26a701-cddc-4ede-8865-c392e781dfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL:\n",
    "\n",
    "with open('data/embeddings/defender_value_embeddings.json', 'r') as f:\n",
    "    value_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69db75-0ac6-445b-a8e8-a5688febf48c",
   "metadata": {},
   "source": [
    "### Step #5: Creating the Entire Pipeline\n",
    "\n",
    "Now that we have the necessary components (Table Embeddings, Value Embeddings, Few-Shot Embeddings, we can build out the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67f1b4-463c-4be8-929f-1227e9fb08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Evaluation Dataset:\n",
    "eval_df = pd.read_json(path_or_buf='data/evaluation/Defender_Evaluation.jsonl', lines=True)\n",
    "\n",
    "# If you only plan on testing a subset of queries, then alter these accordingly:\n",
    "queries = list(eval_df['context'])\n",
    "baselines = list(eval_df['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4abe27-af26-40ae-b6ae-4eaf8a1115cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Results get stored in here:\n",
    "df = pd.DataFrame()\n",
    "query_count = 0\n",
    "failed_queries = []\n",
    "\n",
    "with open('data/DataCatalogs/Defender_DataCatalog.yml', 'r') as file:\n",
    "    defender_catalog = yaml.safe_load(file)\n",
    "\n",
    "elapsed_time = []\n",
    "inputs = []\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "if mode == 'genai-client':\n",
    "    iterations = 5\n",
    "else:\n",
    "    iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8832f75-4076-4be0-8c13-cdbcfa932b49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,iterations):\n",
    "\n",
    "    for query_prompt in queries:\n",
    "        \n",
    "        llm_results_gemini = []\n",
    "        fail = False\n",
    "        \n",
    "        # Generating the embedding for the query:\n",
    "        query_response = get_query_embedding(query_prompt)\n",
    "        \n",
    "        # Get the relevant tables to the query:\n",
    "        defender_embedding_vals = list(table_embeddings.values())\n",
    "        cosine_similarities = [cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry).reshape(1,-1)) for entry in defender_embedding_vals]\n",
    "        \n",
    "        cosine_similarities_vals = [float(entry) for entry in cosine_similarities]\n",
    "        top_9_idx = np.argsort(cosine_similarities_vals)[-9:]\n",
    "        table_lst = list(table_embeddings.keys())\n",
    "    \n",
    "        filtered_tables = []\n",
    "        for idx in top_9_idx:\n",
    "            filtered_tables.append(table_lst[idx])\n",
    "        \n",
    "        # Get the relevant columns to the query:\n",
    "        # with open('../../../../tmp/defender_value_embeddings_new_two.json', 'r') as f:\n",
    "        #     value_embeddings = json.load(f)\n",
    "        \n",
    "        relevant_columns = dict()\n",
    "    \n",
    "        for k in filtered_tables:\n",
    "            filter_lst = [entry for entry in list(value_embeddings.keys()) if k in entry]\n",
    "            sub_dict = {key: value_embeddings[key] for key in filter_lst}\n",
    "            \n",
    "            cosine_similarities = []\n",
    "            for key in sub_dict:\n",
    "                cosine_similarity_val = cosine_similarity(np.array(value_embeddings[key]).reshape(1,-1), \n",
    "                                                          np.array(query_response).reshape(1,-1))\n",
    "                cosine_similarities.append(cosine_similarity_val[0].item())\n",
    "    \n",
    "            top_5_cols_idx = np.argsort(cosine_similarities)[-5:]\n",
    "    \n",
    "            col_lst = list(sub_dict.keys())\n",
    "            relevant_cols = []\n",
    "            for idx in top_5_cols_idx:\n",
    "                relevant_columns[col_lst[idx]] = cosine_similarities[idx]\n",
    "        \n",
    "        # Top 5 Values:\n",
    "        final_vals = list(dict(sorted(relevant_columns.items(), key=lambda item: item[1], reverse=True)).keys())[0:5]\n",
    "        final_vals_revised = []\n",
    "        for entry in final_vals:\n",
    "            try:\n",
    "                final_vals_revised.append(re.search(r'Value Name:(.*)', entry).group(1))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        # Filter Few-Shot by Top t tables:\n",
    "        filtered_fsdb = [entry for entry in fsdb if len(set(entry['tables']).intersection(set(filtered_tables))) > 0]\n",
    "        \n",
    "        # Semantic Similarity Matching:\n",
    "        # nlq, f = 2\n",
    "    \n",
    "        fsdb_embeddings = []\n",
    "        fsdb_count = 0\n",
    "    \n",
    "        for entry in filtered_fsdb:\n",
    "            try:\n",
    "                fsdb_response = client.models.embed_content(model = 'text-embedding-004',\n",
    "                                                            contents = f\"{entry['nlq']}\")\n",
    "            except:\n",
    "                print('Error found - retrying again')\n",
    "                time.sleep(60)\n",
    "                fsdb_response = client.models.embed_content(model = 'text-embedding-004',\n",
    "                                                            contents = f\"{entry['nlq']}\")\n",
    "                \n",
    "            fsdb_embeddings.append({'NLQ': entry['nlq'], 'Embedding': fsdb_response.embeddings[0].values, 'KQL': entry['kql']})\n",
    "            fsdb_count = fsdb_count + 1\n",
    "            # print(f\"FSDB Embeddings Processed: {fsdb_count}\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        cosine_similarities_fsdb = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': float(cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry['Embedding']).reshape(1,-1)))} for entry in fsdb_embeddings]\n",
    "    \n",
    "        # Sort and find the Top 2 NLQ Entries:\n",
    "        cosine_similarities_fsdb_sorted = sorted(cosine_similarities_fsdb, key=lambda x: x['Similarity'], reverse=True)[0:2]\n",
    "    \n",
    "        # Filter KQL Queries:\n",
    "        cosine_similarities_fsdb_sorted = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': entry['Similarity']} for entry in cosine_similarities_fsdb_sorted]\n",
    "        \n",
    "        SCHEMA_PLACEHOLDER = \"\"\n",
    "        EXAMPLES_PLACEHOLDER = \"\"\n",
    "        USER_PLACEHOLDER = f\"NLQ: {query_prompt} \\n + KQL:\"\n",
    "        \n",
    "        with open(prompt_template_path, 'r') as f:\n",
    "            txt = f.read()\n",
    "    \n",
    "        # Add Table and Schema Information:\n",
    "        for k in filtered_tables:\n",
    "            temp_col_lst = []\n",
    "            table = k\n",
    "            \n",
    "            for entry in defender_catalog:\n",
    "                if entry['Name'] == table:\n",
    "                    temp_col_lst = [subentry['Name'] for subentry in entry['Columns']]\n",
    "                    \n",
    "            #cols = relevant_columns[key]\n",
    "            col_combined = \", \".join(temp_col_lst)\n",
    "            SCHEMA_PLACEHOLDER += f\"Table: {k}, Columns: {col_combined}\"\n",
    "            SCHEMA_PLACEHOLDER += \"\\n\"\n",
    "        \n",
    "        txt = txt.replace('{{SCHEMA_PLACEHOLDER}}', SCHEMA_PLACEHOLDER)\n",
    "        \n",
    "        # Add Value Information (only if value_placeholder is set to True):\n",
    "        if value_placeholder:\n",
    "            txt = txt.replace('{{VALUES_PLACEHOLDER}}', str(final_vals_revised))\n",
    "        \n",
    "        # Add Examples:\n",
    "        for entry in cosine_similarities_fsdb_sorted:\n",
    "            EXAMPLES_PLACEHOLDER += f\"NLQ: {entry['NLQ']} \\n + KQL: {entry['KQL']}\"\n",
    "            EXAMPLES_PLACEHOLDER += \"\\n\"\n",
    "    \n",
    "        txt = txt.replace('{{EXAMPLES_PLACEHOLDER}}', EXAMPLES_PLACEHOLDER)\n",
    "        txt = txt.replace('{{USER_REQUEST_PLACEHOLDER}}', USER_PLACEHOLDER)\n",
    "        \n",
    "        if mode == 'genai-client':\n",
    "            try:\n",
    "                start = time.time()\n",
    "                response = client.models.generate_content(model = model_name, contents = txt)\n",
    "                end = time.time()\n",
    "            except:\n",
    "                print(f\"Error with {query_count}\")\n",
    "                failed_queries.append(query_count)\n",
    "                fail = True\n",
    "                time.sleep(60)\n",
    "        \n",
    "        elif mode == 'openai':\n",
    "            start = time.time()\n",
    "            response = client_two.chat.completions.create(\n",
    "                model = model_name,\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": txt}\n",
    "                ],\n",
    "            ) \n",
    "            end = time.time()\n",
    "            \n",
    "        # Add the time:\n",
    "        elapsed_time.append((end-start))\n",
    "        \n",
    "        # Add the tokens:\n",
    "        inputs.append(txt)\n",
    "\n",
    "        if mode == 'genai-client':\n",
    "            if fail == False:\n",
    "                llm_kql_query = response.text\n",
    "            else:\n",
    "                llm_kql_query = \"Error\"\n",
    "        elif mode == 'openai':\n",
    "            llm_kql_query = response.choices[0].message.content\n",
    "            \n",
    "        df = pd.concat([df, pd.DataFrame([{'NLQ': query_prompt, 'LLM-KQL': llm_kql_query}])], ignore_index=True)\n",
    "        query_count += 1\n",
    "        \n",
    "        print(llm_kql_query)\n",
    "        print(f\"Queries processed: {query_count}\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc756602-80d0-4551-83fe-02478209797c",
   "metadata": {},
   "source": [
    "### Step #6: Latency Output\n",
    "\n",
    "Prints out the average latency for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9a573-0375-416f-b125-588f8da6301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(elapsed_time)/len(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0dfa8-2a09-4923-a5f7-ca37093d5ad2",
   "metadata": {},
   "source": [
    "### Step #7: Cost Analysis:\n",
    "\n",
    "Prints out the average cost for running all queries. In the following cell we calculate the costs of running the model. You will need to change the input and output token costs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682226b8-8711-4f26-8d11-eb11b5689955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You need to update the costs here:\n",
    "llm_input_cost_per_million = 0.10\n",
    "llm_output_cost_per_million = 0.40\n",
    "\n",
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "\n",
    "if mode == 'openai':\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "    for entry in inputs:\n",
    "        input_tokens = input_tokens + len(encoding.encode(entry))\n",
    "    \n",
    "    for entry in df['LLM-KQL']:\n",
    "        output_tokens = output_tokens + len(encoding.encode(entry))\n",
    "\n",
    "    cost = ((llm_input_cost_per_million * input_tokens)/1000000) + ((llm_output_cost_per_million * output_tokens)/1000000)\n",
    "    avg_cost = cost/iterations\n",
    "    \n",
    "    print(f\"Average Total Cost: ${avg_cost}\")\n",
    "\n",
    "elif mode == 'genai-client':\n",
    "\n",
    "    for entry in inputs:\n",
    "        input_tokens = input_tokens + client.models.count_tokens(model=model_name, contents=str(entry)).total_tokens\n",
    "    \n",
    "    for entry in df['LLM-KQL']:\n",
    "        output_tokens = output_tokens + client.models.count_tokens(model=model_name, contents=str(entry)).total_tokens\n",
    "\n",
    "    cost = ((llm_input_cost_per_million * input_tokens)/1000000) + ((llm_output_cost_per_million * output_tokens)/1000000)\n",
    "    avg_cost = cost/iterations\n",
    "    \n",
    "    print(f\"Average Total Cost: ${avg_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173701ef-8195-4d0c-ac88-8e63c66518df",
   "metadata": {},
   "source": [
    "### Step #8: Cleaning + Query Refiner\n",
    "\n",
    "Next we must feed the results through the Query Refiner. In order to do this, we must clean up the results to **only include** the first provided KQL query. Please note that manual revision to remove all extra results might also be needed, and additional regex logic may need to be edited/added depending on the model that you are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f94bdd-fc0f-4233-b40c-14abd4b188e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_results = []\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        query = re.search(r'(?:~~~|```)(?:kusto|kql)(.*)(?:~~~|```)', row['LLM-KQL'], flags=re.DOTALL).group(1)\n",
    "        extracted_results.append(query)\n",
    "    except:\n",
    "        extracted_results.append(row['LLM-KQL'])\n",
    "\n",
    "df['LLM-KQL-Extracted'] = extracted_results\n",
    "\n",
    "# Save results to .csv in \"temp\" folder, you can rename this folder as you wish:\n",
    "revised_model_name = model_name\n",
    "os.makedirs('temp', exist_ok = True)\n",
    "\n",
    "baseline = []\n",
    "for i in range(0,iterations):\n",
    "    baseline = baseline + baselines\n",
    "\n",
    "df['baseline'] = baseline\n",
    "df.to_csv(f'temp/{revised_model_name}-cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e395b-5e77-4d21-ad0b-524a91b29238",
   "metadata": {},
   "source": [
    "Before we run the query parser, you will need to manually revise the KQL queries so that for each entry, there is only one KQL query. This is because of the LLM/SLM tendency to hallucinate and produce its own Natural Language Queries (NLQ) and respective KQL responses. Although we have tried to mitigate these circumstances to the best of our ability, it is not perfect. **PLEASE** do review the results in the .csv before going forward to ensure that only one KQL query is provided in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d31db-13f6-4c33-ada2-2a23a174a7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../Query_Refiner/query_parser_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = f'temp/{revised_model_name}-cleaned.csv'\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "stored_folder = \"temp\"\n",
    "\n",
    "!python {runner} {file_of_interest} {revised_model_name} {stored_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b756716-3d08-4899-ab81-81f064e4cfac",
   "metadata": {},
   "source": [
    "### Step #9: Metrics\n",
    "\n",
    "The following snippets will obtain all metrics for the results that you have just calculated. Please note that full path **must** be specified for ```file_of_interest``` and ```folder```. If you need help specifying the full path, use the ```!pwd``` command in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1036d28-874e-496c-bec2-3948e62d478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../offline_metrics_pipeline/offline-metrics-pipeline/offline_metrics_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "folder = \"\"\n",
    "results_file = \"testing.csv\"\n",
    "\n",
    "!python {runner} {file_of_interest} {folder} {results_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
