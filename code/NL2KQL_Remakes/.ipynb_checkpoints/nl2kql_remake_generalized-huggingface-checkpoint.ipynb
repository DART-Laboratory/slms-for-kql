{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9474ca2-a4ce-4a2c-8712-3531c608311f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NL2KQL (Generalized):\n",
    "\n",
    "The following notebook is a replication of [NL2KQL](https://arxiv.org/pdf/2404.02933). There are multiple main components involved in this pipeline:\n",
    "\n",
    "- Schema Refiner\n",
    "- Few-Shot Selector\n",
    "- Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8353d0-a9e8-4cd7-a40a-204db445c165",
   "metadata": {},
   "source": [
    "### Step #1: Import Necessary Packages:\n",
    "\n",
    "Import all the packages that are needed to run the notebook. To install packages, you will need to run the following command in either your Terminal or in any of the cells:\n",
    "\n",
    "```!pip install -r ../requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0371-371c-4758-b990-dd3853576ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Necessary Packages:\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning-based packages:\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from helpers import *\n",
    "\n",
    "import pathlib\n",
    "import textwrap\n",
    "import time\n",
    "import pandas\n",
    "import json\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bd4b1-c389-4092-9340-93449fb6eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your HuggingFace Token here:\n",
    "TOKEN = token_config['huggingface']['token']\n",
    "\n",
    "# Put the path to your model here (either HuggingFace or Fine-Tuned):\n",
    "model_name = \"\"\n",
    "\n",
    "# Path to prompt template:\n",
    "prompt_template_path = \"\"\n",
    "\n",
    "# Should be a boolean (True/False):\n",
    "value_placeholder = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e5feb-267d-492a-8435-66611b722401",
   "metadata": {},
   "source": [
    "### Step #2: Creating the Few-Shot Embedding Store Database\n",
    "\n",
    "The following box creates a Few-Shot Embedding Store Database. The purpose of this database is to create a variety of KQL examples that can be provided to the LLM in order to improve KQL code generation accuracy. Note that if an FSDB database has already been created, you do not need to run the block below (running the block below when an FSDB database exists will result in a message that says \"FSDB already exists for Defender\"). Instead run the second block below, which reads in the Defender FSDB database. The first block below is **commented**, if you need to make changes to the FSDB, then uncomment and run the cell.\n",
    "\n",
    "If you need to make any changes to how you generate the FSDB, see the ```helpers.py``` file.\n",
    "\n",
    "NOTE: For the purposes of this project, we focus exclusively on Microsoft Defender for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96e726-208d-491d-ba3b-fc4a435dbefd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# themes = [\n",
    "#     \"Explore: Look for signs or hints of a security attack\",\n",
    "#     \"Expansion: Searches for additional contextual understanding\",\n",
    "#     \"Detect: Look for events related to a security attack\",\n",
    "#     \"Remediate: Identify events for a given entity or asset\",\n",
    "#     \"Report: Provide summary statistics for reporting\"\n",
    "# ]\n",
    "\n",
    "# schema_file = \"defender_fsdb_new.json\"\n",
    "# fsdb = generate_fsdb(themes, schema_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fd3e8-637b-46f0-a699-9181bd456981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fsdb/defender_fsdb.json', 'r') as f:\n",
    "    fsdb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fda19a-db08-4496-951d-6e4d7e63eddd",
   "metadata": {},
   "source": [
    "### Step #3: Generating the Table Embedding Store (Semantic Data Catalog)\n",
    "\n",
    "As part of the Semantic Data Catalog, there are two different types of embeddings: Table Embeddings and Value Embeddings. In the next few steps, we will first make generate a table embedding dictionary and later build a value embedding dictionary to simulate the table embedding and value embedding stores discussed in NL2KQL:\n",
    "\n",
    "Note: If a Table Embeddings file has already been created, then run the third block below instead to load the Table Embeddings directly from a json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef04fe3-164c-45bc-804b-8ed1152b8403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dont run\n",
    "with open('data/miscellaneous/defender.yml', 'r') as file:\n",
    "    defender_information = yaml.safe_load(file)\n",
    "    \n",
    "defender_embeddings = generate_table_embeddings(defender_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615707e8-51b0-4440-9c51-adf7972f29b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dont run\n",
    "with open('table_embeddings.json', 'w') as f:\n",
    "    json.dump(defender_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4b9e9-ed66-4803-90e3-e5eb559cf459",
   "metadata": {},
   "source": [
    "If you have already run the two code block snippets above before, then run the following block instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776b6a2-2787-4a3f-a8d1-83ea27283d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/defender_table_embeddings.json', 'r') as f:\n",
    "    table_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d45f1-4f7d-4655-9761-3a815b8ccd23",
   "metadata": {},
   "source": [
    "### Step #4: Generating the Value Embedding Stores (Semantic Data Catalog)\n",
    "\n",
    "To preserve efficiency, we then find the embeddings of the columns of the filtered tables. Normally it would be efficient to store them all at once, but for this replication we just aim to generate the embeddings once we have the desired tables. We are bound by requests per day from the queries (1,500 per day) when querying Google Gemini 2.0, which is why we find the tables first, and then their respective embeddings.\n",
    "\n",
    "Because the process takes a bit of time to generate, we do not supply the code here used to generate the embeddings (that is included in a separate notebook). Instead, just load the value embeddings through the block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26a701-cddc-4ede-8865-c392e781dfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/defender_value_embeddings.json', 'r') as f:\n",
    "    value_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69db75-0ac6-445b-a8e8-a5688febf48c",
   "metadata": {},
   "source": [
    "### Step #5: Creating the Entire Pipeline\n",
    "\n",
    "Now that we have the necessary components (Table Embeddings, Value Embeddings, Few-Shot Embeddings, we can build out the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67f1b4-463c-4be8-929f-1227e9fb08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Evaluation Dataset:\n",
    "eval_df = pd.read_json(path_or_buf='data/evaluation/Defender_Evaluation.jsonl', lines=True)\n",
    "\n",
    "# If you only plan on testing a subset of queries, then alter these accordingly:\n",
    "queries = list(eval_df['context'])\n",
    "baselines = list(eval_df['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4abe27-af26-40ae-b6ae-4eaf8a1115cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Results get stored in here:\n",
    "df = pd.DataFrame()\n",
    "query_count = 0\n",
    "\n",
    "with open('data/DataCatalogs/Defender_DataCatalog.yml', 'r') as file:\n",
    "    defender_catalog = yaml.safe_load(file)\n",
    "\n",
    "model_args = {\n",
    "    \"token\": TOKEN,\n",
    "    \"torch_dtype\": torch.float32,\n",
    "    \"device_map\": \"auto\",\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **model_args).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=TOKEN)\n",
    "pipeline = transformers.pipeline(\"text-generation\",\n",
    "                                 model=model,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 torch_dtype=model_args[\"torch_dtype\"])\n",
    "\n",
    "elapsed_time = []\n",
    "tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8832f75-4076-4be0-8c13-cdbcfa932b49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "\n",
    "    for query_prompt in queries:\n",
    "        \n",
    "        llm_results_gemini = []\n",
    "        \n",
    "        # Generating the embedding for the query:\n",
    "        query_response = get_query_embedding(query_prompt)\n",
    "        \n",
    "        # Get the relevant tables to the query:\n",
    "        defender_embedding_vals = list(table_embeddings.values())\n",
    "        cosine_similarities = [cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry).reshape(1,-1)) for entry in defender_embedding_vals]\n",
    "        \n",
    "        cosine_similarities_vals = [float(entry) for entry in cosine_similarities]\n",
    "        top_9_idx = np.argsort(cosine_similarities_vals)[-9:]\n",
    "        table_lst = list(table_embeddings.keys())\n",
    "    \n",
    "        filtered_tables = []\n",
    "        for idx in top_9_idx:\n",
    "            filtered_tables.append(table_lst[idx])\n",
    "        \n",
    "        relevant_columns = dict()\n",
    "    \n",
    "        for k in filtered_tables:\n",
    "            filter_lst = [entry for entry in list(value_embeddings.keys()) if k in entry]\n",
    "            sub_dict = {key: value_embeddings[key] for key in filter_lst}\n",
    "            \n",
    "            cosine_similarities = []\n",
    "            for key in sub_dict:\n",
    "                cosine_similarity_val = cosine_similarity(np.array(value_embeddings[key]).reshape(1,-1), \n",
    "                                                          np.array(query_response).reshape(1,-1))\n",
    "                cosine_similarities.append(cosine_similarity_val[0].item())\n",
    "    \n",
    "            top_5_cols_idx = np.argsort(cosine_similarities)[-5:]\n",
    "    \n",
    "            col_lst = list(sub_dict.keys())\n",
    "            relevant_cols = []\n",
    "            for idx in top_5_cols_idx:\n",
    "                relevant_columns[col_lst[idx]] = cosine_similarities[idx]\n",
    "        \n",
    "        # Top 5 Values:\n",
    "        final_vals = list(dict(sorted(relevant_columns.items(), key=lambda item: item[1], reverse=True)).keys())[0:5]\n",
    "        final_vals_revised = []\n",
    "        for entry in final_vals:\n",
    "            try:\n",
    "                final_vals_revised.append(re.search(r'Value Name:(.*)', entry).group(1))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        # Filter Few-Shot by Top t tables:\n",
    "        filtered_fsdb = [entry for entry in fsdb if len(set(entry['tables']).intersection(set(filtered_tables))) > 0]\n",
    "        \n",
    "        # Semantic Similarity Matching:\n",
    "        # nlq, f = 2\n",
    "    \n",
    "        fsdb_embeddings = []\n",
    "        fsdb_count = 0\n",
    "    \n",
    "        for entry in filtered_fsdb:\n",
    "            fsdb_response = client.models.embed_content(model = 'text-embedding-004',\n",
    "                                                        contents = f\"{entry['nlq']}\")\n",
    "    \n",
    "            fsdb_embeddings.append({'NLQ': entry['nlq'], 'Embedding': fsdb_response.embeddings[0].values, 'KQL': entry['kql']})\n",
    "            fsdb_count = fsdb_count + 1\n",
    "            # print(f\"FSDB Embeddings Processed: {fsdb_count}\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        cosine_similarities_fsdb = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': float(cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry['Embedding']).reshape(1,-1)))} for entry in fsdb_embeddings]\n",
    "    \n",
    "        # Sort and find the Top 2 NLQ Entries:\n",
    "        cosine_similarities_fsdb_sorted = sorted(cosine_similarities_fsdb, key=lambda x: x['Similarity'], reverse=True)[0:2]\n",
    "    \n",
    "        # Filter KQL Queries:\n",
    "        cosine_similarities_fsdb_sorted = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': entry['Similarity']} for entry in cosine_similarities_fsdb_sorted]\n",
    "        \n",
    "        SCHEMA_PLACEHOLDER = \"\"\n",
    "        EXAMPLES_PLACEHOLDER = \"\"\n",
    "        USER_PLACEHOLDER = f\"NLQ: {query_prompt} \\n + KQL:\"\n",
    "        \n",
    "        with open(prompt_template_path, 'r') as f:\n",
    "            txt = f.read()\n",
    "    \n",
    "        # Add Table and Schema Information:\n",
    "        for k in filtered_tables:\n",
    "            temp_col_lst = []\n",
    "            table = k\n",
    "            \n",
    "            for entry in defender_catalog:\n",
    "                if entry['Name'] == table:\n",
    "                    temp_col_lst = [subentry['Name'] for subentry in entry['Columns']]\n",
    "                    \n",
    "            #cols = relevant_columns[key]\n",
    "            col_combined = \", \".join(temp_col_lst)\n",
    "            SCHEMA_PLACEHOLDER += f\"Table: {k}, Columns: {col_combined}\"\n",
    "            SCHEMA_PLACEHOLDER += \"\\n\"\n",
    "        \n",
    "        txt = txt.replace('{{SCHEMA_PLACEHOLDER}}', SCHEMA_PLACEHOLDER)\n",
    "        \n",
    "        # Add Value Information (only if value_placeholder is set to True):\n",
    "        if value_placeholder:\n",
    "            txt = txt.replace('{{VALUES_PLACEHOLDER}}', str(final_vals_revised))\n",
    "        \n",
    "        # Add Examples:\n",
    "        for entry in cosine_similarities_fsdb_sorted:\n",
    "            EXAMPLES_PLACEHOLDER += f\"NLQ: {entry['NLQ']} \\n + KQL: {entry['KQL']}\"\n",
    "            EXAMPLES_PLACEHOLDER += \"\\n\"\n",
    "    \n",
    "        txt = txt.replace('{{EXAMPLES_PLACEHOLDER}}', EXAMPLES_PLACEHOLDER)\n",
    "        txt = txt.replace('{{USER_REQUEST_PLACEHOLDER}}', USER_PLACEHOLDER)\n",
    "        \n",
    "        start = time.time()\n",
    "        response = pipeline(txt, do_sample=True, return_full_text = False)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Add the time:\n",
    "        elapsed_time.append((end-start))\n",
    "        \n",
    "        # Add the tokens:\n",
    "        input_tokens = tokenizer(txt)\n",
    "        tokens = tokens + len(input_tokens['input_ids'])\n",
    "        \n",
    "        llm_kql_query = response[0].get(\"generated_text\")\n",
    "        df = pd.concat([df, pd.DataFrame([{'NLQ': query_prompt, 'LLM-KQL': llm_kql_query}])], ignore_index=True)\n",
    "        query_count += 1\n",
    "        \n",
    "        print(llm_kql_query)\n",
    "        print(f\"Queries processed: {query_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc756602-80d0-4551-83fe-02478209797c",
   "metadata": {},
   "source": [
    "### Step #6: Latency Output\n",
    "\n",
    "Prints out the average latency for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9a573-0375-416f-b125-588f8da6301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(elapsed_time)/len(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc021458-67d3-4ec1-83c8-916cd9dee7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0dfa8-2a09-4923-a5f7-ca37093d5ad2",
   "metadata": {},
   "source": [
    "### Step #7: Cost Analysis:\n",
    "\n",
    "Prints out the average cost for running all queries. In the following cell we calculate the costs of running the model. You will need to change the input and output token costs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682226b8-8711-4f26-8d11-eb11b5689955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to update the costs here:\n",
    "\n",
    "llm_input_cost_per_million =0.15\n",
    "llm_output_cost_per_million = 0.15\n",
    "output_tokens = 0\n",
    "\n",
    "for entry in df['LLM-KQL']:\n",
    "    output_tokens = output_tokens + len(tokenizer(entry)['input_ids'])\n",
    "\n",
    "cost = ((llm_input_cost_per_million * tokens)/1000000) + ((llm_output_cost_per_million * output_tokens)/1000000)\n",
    "avg_cost = cost/5\n",
    "\n",
    "print(f\"Average Total Cost: ${avg_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173701ef-8195-4d0c-ac88-8e63c66518df",
   "metadata": {},
   "source": [
    "### Step #8: Cleaning + Query Refiner\n",
    "\n",
    "Next we must feed the results through the Query Refiner. In order to do this, we must clean up the results to **only include** the first provided KQL query. Please note that manual revision to remove all extra results might also be needed, and additional regex logic may need to be edited/added depending on the model that you are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f94bdd-fc0f-4233-b40c-14abd4b188e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_results = []\n",
    "count = 0\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_kql_deepseek(raw: str) -> str:\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "\n",
    "    text = raw\n",
    "\n",
    "    # Normalize escapes and newlines\n",
    "    if \"\\\\n\" in text and \"\\n\" not in text:\n",
    "        text = text.replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\")\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Strip single outer quotes\n",
    "    if len(text) >= 2 and text[0] == text[-1] and text[0] in (\"'\", '\"'):\n",
    "        text = text[1:-1]\n",
    "\n",
    "    # -------- 1) Remove fence marker lines (but keep their bodies) --------\n",
    "    raw_lines = text.split(\"\\n\")\n",
    "    lines = [ln for ln in raw_lines if not re.match(r\"^\\s*(?:```|~~~)\", ln)]\n",
    "\n",
    "    if not lines:\n",
    "        return text.strip()\n",
    "\n",
    "    # -------- 2) Helper patterns --------\n",
    "    table_only_pat = re.compile(r\"^\\s*[A-Za-z][\\w.]*\\s*$\")\n",
    "    table_with_pipe_pat = re.compile(r\"^\\s*([A-Za-z][\\w.]*)\\s*\\|.*$\")\n",
    "\n",
    "    stop_pat = re.compile(\n",
    "        r\"^\\s*(?:#|//|--|/\\*|\\*/|NLQ:|Reminder\\b|All steps\\b|Step\\s+\\d+\"\n",
    "        r\"|Answer:|Explanation:|Your task\\b|Task\\b|Delivery\\b|Solution:)\",\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    def is_kql_keyword_line(s: str) -> bool:\n",
    "        return bool(\n",
    "            re.match(\n",
    "                r\"^(where|and|or|summarize|extend|project|project-rename\"\n",
    "                r\"|project-away|project-reorder|join|on|lookup\"\n",
    "                r\"|order\\s+by|take|top|limit|render)\\b\",\n",
    "                s.strip(),\n",
    "                flags=re.IGNORECASE,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def is_table_line(line: str) -> bool:\n",
    "        return bool(table_only_pat.match(line))\n",
    "\n",
    "    def is_query_start_line(line: str) -> bool:\n",
    "        s = line.strip()\n",
    "        if \"KQL QUERY GOES HERE\" in s.upper():\n",
    "            return False\n",
    "        if table_with_pipe_pat.match(line):\n",
    "            return True\n",
    "        if is_table_line(line):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_obviously_non_kql(line: str, allow_table: bool = False) -> bool:\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            return True\n",
    "        if \"KQL QUERY GOES HERE\" in s.upper():\n",
    "            return True\n",
    "        if stop_pat.match(line):\n",
    "            return True\n",
    "\n",
    "        # Once inside a query, allow table-only lines (e.g., in join subqueries)\n",
    "        if allow_table and is_table_line(line):\n",
    "            return False\n",
    "\n",
    "        # Lines starting with '|' are always treated as KQL\n",
    "        if s.startswith(\"|\"):\n",
    "            return False\n",
    "\n",
    "        # Parentheses around subqueries / join conditions\n",
    "        if s.startswith(\"(\") or s.startswith(\")\"):\n",
    "            return False\n",
    "\n",
    "        # Lines mentioning join/on/lookup are likely query syntax\n",
    "        if re.search(r\"\\b(join|on|lookup)\\b\", s, flags=re.IGNORECASE):\n",
    "            return False\n",
    "\n",
    "        if is_kql_keyword_line(s):\n",
    "            return False\n",
    "\n",
    "        # Otherwise treat as non-KQL (English, instructions, etc.)\n",
    "        return True\n",
    "\n",
    "    # -------- 3) Find first query start line --------\n",
    "    start_idx = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if \"KQL QUERY GOES HERE\" in s.upper():\n",
    "            continue\n",
    "\n",
    "        if table_with_pipe_pat.match(line):\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "        if is_table_line(line):\n",
    "            # look ahead for evidence of KQL\n",
    "            for look in lines[i + 1 : i + 8]:\n",
    "                if not look.strip():\n",
    "                    continue\n",
    "                if look.lstrip().startswith(\"|\") or is_kql_keyword_line(look):\n",
    "                    start_idx = i\n",
    "                    break\n",
    "            if start_idx is not None:\n",
    "                break\n",
    "\n",
    "    if start_idx is None:\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"|\" in line and re.search(r\"[A-Za-z_][A-Za-z0-9_]*\\s*\\|\", line):\n",
    "                if \"KQL QUERY GOES HERE\" not in line.upper():\n",
    "                    start_idx = i\n",
    "                    break\n",
    "\n",
    "    if start_idx is None:\n",
    "        if \"KQL QUERY GOES HERE\" in text.upper():\n",
    "            return \"\"\n",
    "        return text.strip()\n",
    "\n",
    "    # -------- 4) Collect lines until we hit non-KQL --------\n",
    "    query_lines = []\n",
    "    for j in range(start_idx, len(lines)):\n",
    "        L = lines[j]\n",
    "        if j == start_idx:\n",
    "            query_lines.append(L.rstrip())\n",
    "            continue\n",
    "\n",
    "        # allow_table=True here so we keep inner table lines like \"DeviceFileEvents\"\n",
    "        if is_obviously_non_kql(L, allow_table=True):\n",
    "            break\n",
    "\n",
    "        query_lines.append(L.rstrip())\n",
    "\n",
    "    while query_lines and not query_lines[-1].strip():\n",
    "        query_lines.pop()\n",
    "\n",
    "    if not query_lines:\n",
    "        if \"KQL QUERY GOES HERE\" in text.upper():\n",
    "            return \"\"\n",
    "        return text.strip()\n",
    "\n",
    "    query = \"\\n\".join(query_lines).strip()\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if model_name == 'google/gemma-3-4b-it':\n",
    "        try:\n",
    "            import re\n",
    "    \n",
    "            s = row['LLM-KQL'].lstrip(\" '\")\n",
    "            had_literal_newlines = (\"\\\\n\" in s) and (\"\\n\" not in s)\n",
    "            s_norm = s.replace(\"\\\\n\", \"\\n\") if had_literal_newlines else s\n",
    "            s_norm = s_norm.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    \n",
    "            lines = s_norm.split(\"\\n\")\n",
    "            sep_pat = re.compile(r\"^\\s*(?:~~~|NLQ:|\\+\\s*KQL:|```)\", re.IGNORECASE)\n",
    "            table_pat = re.compile(r\"^\\s*[A-Za-z][\\w.]*\\s*$\")\n",
    "    \n",
    "            # Find first table-ish start line\n",
    "            start = None\n",
    "            for i, line in enumerate(lines):\n",
    "                if table_pat.match(line):\n",
    "                    start = i\n",
    "                    break\n",
    "    \n",
    "            if start is not None:\n",
    "                collected = [lines[start]]\n",
    "                for j in range(start + 1, len(lines)):\n",
    "                    L = lines[j]\n",
    "                    if sep_pat.match(L):\n",
    "                        break\n",
    "                    collected.append(L)\n",
    "                query = \"\\n\".join(collected).strip()\n",
    "            else:\n",
    "                # fallback: first \"+ KQL:\" block\n",
    "                kql_body = []\n",
    "                for i, line in enumerate(lines):\n",
    "                    if re.match(r\"^\\s*\\+\\s*KQL:\", line, re.IGNORECASE):\n",
    "                        remainder = re.sub(r\"^\\s*\\+\\s*KQL:\\s*\", \"\", line, flags=re.IGNORECASE)\n",
    "                        kql_body = [remainder] if remainder else []\n",
    "                        for j in range(i + 1, len(lines)):\n",
    "                            L = lines[j]\n",
    "                            if sep_pat.match(L):\n",
    "                                break\n",
    "                            kql_body.append(L)\n",
    "                        break\n",
    "                query = \"\\n\".join(kql_body).strip() if kql_body else s_norm.strip()\n",
    "    \n",
    "            # ðŸ§¹ Remove markdown or inline comments starting with '#'\n",
    "            query = \"\\n\".join(\n",
    "                [ln for ln in query.split(\"\\n\") if not ln.strip().startswith(\"#\")]\n",
    "            ).strip()\n",
    "    \n",
    "            if had_literal_newlines:\n",
    "                query = query.replace(\"\\n\", \"\\\\n\")\n",
    "    \n",
    "            extracted_results.append(query)\n",
    "    \n",
    "        except Exception:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "\n",
    "    if model_name == 'google/gemma-3-1b-it':\n",
    "        try:\n",
    "            query = row['LLM-KQL'].split(\"~~~\")[0]\n",
    "            extracted_results.append(query)\n",
    "        except:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "\n",
    "    if model_name == 'microsoft/phi-4':\n",
    "        try:\n",
    "            lst = row['LLM-KQL'].split(\"~~~\")\n",
    "            if '```' in lst[0]:\n",
    "                lst = lst[0].split('```')\n",
    "                result = lst[0].split('\\n\\n')\n",
    "                extracted_results.append(result[0])\n",
    "            else:\n",
    "                result = lst[0].split('\\n\\n')\n",
    "                extracted_results.append(result[0])\n",
    "        except:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "\n",
    "    if model_name == 'microsoft/phi-4-mini-instruct':\n",
    "        try:\n",
    "            lst = row['LLM-KQL'].split(\"~~~\")\n",
    "            if '```' in lst[0]:\n",
    "                lst = lst[0].split('```')\n",
    "                result = lst[0].split('\\n\\n')\n",
    "                extracted_results.append(result[0])\n",
    "            else:\n",
    "                result = lst[0].split('\\n\\n')\n",
    "                extracted_results.append(result[0])\n",
    "        except:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "            \n",
    "    if model_name == 'Qwen/Qwen2.5-7B-Instruct-1M':\n",
    "        try:\n",
    "            text = row['LLM-KQL']\n",
    "    \n",
    "            m = re.search(\n",
    "                r'(?:```|~~~)\\s*(?:kusto|kql)\\s*\\n(.*?)(?:```|~~~)',\n",
    "                text,\n",
    "                flags=re.IGNORECASE | re.DOTALL\n",
    "            )\n",
    "    \n",
    "            if m:\n",
    "                query = m.group(1).strip()\n",
    "            else:\n",
    "                m2 = re.search(\n",
    "                    r'(?:^|\\n)KQL:\\s*.*?(?:\\n+)([A-Za-z0-9_]+\\n\\|[\\s\\S]*?)(?=\\n{2,}|$)',\n",
    "                    text,\n",
    "                    flags=re.IGNORECASE\n",
    "                )\n",
    "                if m2:\n",
    "                    query = m2.group(1).strip()\n",
    "                else:\n",
    "                    m3 = re.search(\n",
    "                        r'([A-Za-z0-9_]+\\n\\|[^\\n]+(?:\\n\\|[^\\n]+)*)',\n",
    "                        text\n",
    "                    )\n",
    "                    query = m3.group(1).strip() if m3 else text.strip()\n",
    "    \n",
    "            extracted_results.append(query)\n",
    "        except Exception:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "        \n",
    "    if model_name == 'deepseek-ai/deepseek-coder-6.7b-instruct':\n",
    "        try:\n",
    "            query = extract_kql_deepseek(row['LLM-KQL'] or \"\")\n",
    "            extracted_results.append(query)\n",
    "        except Exception:\n",
    "            extracted_results.append(row['LLM-KQL'])\n",
    "\n",
    "df['LLM-KQL-Extracted'] = extracted_results\n",
    "\n",
    "# Save results to .csv in temp folder:\n",
    "revised_model_name = re.search(r'\\/(.*)', model_name, flags=re.DOTALL).group(1)\n",
    "# revised_model_name = \"deepseek_finetuned_model_supervised\"\n",
    "os.makedirs('temp', exist_ok = True)\n",
    "\n",
    "baseline = []\n",
    "for i in range(0,5):\n",
    "    baseline = baseline + baselines\n",
    "\n",
    "df['baseline'] = baseline\n",
    "df.to_csv(f'temp/{revised_model_name}-cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e395b-5e77-4d21-ad0b-524a91b29238",
   "metadata": {},
   "source": [
    "Before we run the query parser, you will need to manually revise the KQL queries so that for each entry, there is only one KQL query. This is because of the LLM/SLM tendency to hallucinate and produce its own Natural Language Queries (NLQ) and respective KQL responses. Although we have tried to mitigate these circumstances to the best of our ability, it is not perfect. **PLEASE** do review the results in the .csv before going forward to ensure that only one KQL query is provided in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d31db-13f6-4c33-ada2-2a23a174a7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "#note: you may need to change the parser runner file for some cases\n",
    "runner = \"../Query_Refiner/query_parser_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "\n",
    "# This should point to where your .csv is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "stored_folder = \"\"\n",
    "\n",
    "!python {runner} {file_of_interest} {revised_model_name} {stored_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b756716-3d08-4899-ab81-81f064e4cfac",
   "metadata": {},
   "source": [
    "### Step #9: Metrics\n",
    "\n",
    "The following snippets will obtain all metrics for the results that you have just calculated. The following snippets will obtain all metrics for the results that you have just calculated. Please note that full path **must** be specified for ```file_of_interest``` and ```folder```. If you need help specifying the full path, use the ```!pwd``` command in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1036d28-874e-496c-bec2-3948e62d478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../offline_metrics_pipeline/offline-metrics-pipeline/offline_metrics_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "folder = \"\"\n",
    "results_file = \"results.csv\"\n",
    "\n",
    "!python {runner} {file_of_interest} {folder} {results_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
