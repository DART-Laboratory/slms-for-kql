{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9474ca2-a4ce-4a2c-8712-3531c608311f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NL2KQL (SLM) Replication Notebook (Gemma-3-4B w/Oracle):\n",
    "\n",
    "The following notebook is a replication of [NL2KQL](https://arxiv.org/pdf/2404.02933) using Gemma 3 (4B). Gemma-3 4B is a powerful small language models developed by Google Deepmind. We choose Google Gemini 2.0 Flash as the oracle LLM, as this model provides free API access and strong inference capabilities. There are multiple main components involved in this pipeline:\n",
    "\n",
    "- Schema Refiner\n",
    "- Few-Shot Selector\n",
    "- Prompt Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8353d0-a9e8-4cd7-a40a-204db445c165",
   "metadata": {},
   "source": [
    "### Step #1: Import Necessary Packages:\n",
    "\n",
    "The following packages are needed in order to run the pipeline. To install packages, you will need to run the following command in either your Terminal or in any of the cells:\n",
    "\n",
    "```!pip install -r ../requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0371-371c-4758-b990-dd3853576ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Necessary Packages:\n",
    "\n",
    "import yaml\n",
    "from google import genai as genai_client\n",
    "from google.genai.types import GenerateContentConfig\n",
    "from google.genai import types\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import pathlib\n",
    "import textwrap\n",
    "import time\n",
    "import pandas\n",
    "import json\n",
    "import random\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94178636-9b97-4c6a-8285-c0fd59cbc23d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify your HuggingFace Token here:\n",
    "TOKEN = token_config['huggingface']['token']\n",
    "\n",
    "# Replace this with your own Google token:\n",
    "API_KEY = token_config['genai']['token']\n",
    "\n",
    "# Path to prompt template:\n",
    "prompt_template_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e5feb-267d-492a-8435-66611b722401",
   "metadata": {},
   "source": [
    "### Step #2: Creating the Few-Shot Embedding Store Database\n",
    "\n",
    "The following box creates a Few-Shot Embedding Store Database. The purpose of this database is to create a variety of KQL examples that can be provided to the LLM in order to improve KQL code generation accuracy. Note that if an FSDB database has already been created, you do not need to run the block below (running the block below when an FSDB database exists will result in a message that says \"FSDB already exists for Defender\"). Instead run the second block below, which reads in the Defender FSDB database.\n",
    "\n",
    "If you need to make any changes to how you generate the FSDB, see the ```helpers.py``` file.\n",
    "\n",
    "NOTE: For the purposes of this project, we focus exclusively on Microsoft Defender for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96e726-208d-491d-ba3b-fc4a435dbefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# themes = [\n",
    "#     \"Explore: Look for signs or hints of a security attack\",\n",
    "#     \"Expansion: Searches for additional contextual understanding\",\n",
    "#     \"Detect: Look for events related to a security attack\",\n",
    "#     \"Remediate: Identify events for a given entity or asset\",\n",
    "#     \"Report: Provide summary statistics for reporting\"\n",
    "# ]\n",
    "\n",
    "# schema_file = \"defender_fsdb_new.json\"\n",
    "# fsdb = generate_fsdb(themes, schema_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fd3e8-637b-46f0-a699-9181bd456981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fsdb/defender_fsdb.json', 'r') as f:\n",
    "    fsdb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fda19a-db08-4496-951d-6e4d7e63eddd",
   "metadata": {},
   "source": [
    "### Step #3: Generating the Table Embedding Store (Semantic Data Catalog)\n",
    "\n",
    "As part of the Semantic Data Catalog, there are two different types of embeddings: Table Embeddings and Value Embeddings. In the next few steps, we will first make generate a table embedding dictionary and later build a value embedding dictionary to simulate the table embedding and value embedding stores discussed in NL2KQL:\n",
    "\n",
    "Note: If a Table Embeddings file has already been created, then run the third block below instead to load the Table Embeddings directly from a json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef04fe3-164c-45bc-804b-8ed1152b8403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/miscellaneous/defender.yml', 'r') as file:\n",
    "    defender_information = yaml.safe_load(file)\n",
    "    \n",
    "defender_embeddings = generate_table_embeddings(defender_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615707e8-51b0-4440-9c51-adf7972f29b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('table_embeddings.json', 'w') as f:\n",
    "    json.dump(defender_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4b9e9-ed66-4803-90e3-e5eb559cf459",
   "metadata": {},
   "source": [
    "If you have already run the two code block snippets above before, then run the following block instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776b6a2-2787-4a3f-a8d1-83ea27283d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/embeddings/defender_table_embeddings.json', 'r') as f:\n",
    "    table_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d45f1-4f7d-4655-9761-3a815b8ccd23",
   "metadata": {},
   "source": [
    "### Step #4: Generating the Value Embedding Stores (Semantic Data Catalog)\n",
    "\n",
    "To preserve efficiency, we then find the embeddings of the columns of the filtered tables. Normally it would be efficient to store them all at once, but for this replication we just aim to generate the embeddings once we have the desired tables. We are bound by requests per day from the queries (1,500 per day) when querying Google Gemini 2.0, which is why we find the tables first, and then their respective embeddings.\n",
    "\n",
    "Because the process takes a bit of time to generate, we do not supply the code here used to generate the embeddings (that is included in a separate notebook). Instead, just load the value embeddings through the block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26a701-cddc-4ede-8865-c392e781dfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/defender_value_embeddings.json', 'r') as f:\n",
    "    value_embeddings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69db75-0ac6-445b-a8e8-a5688febf48c",
   "metadata": {},
   "source": [
    "### Step #5: Creating the Entire Pipeline\n",
    "\n",
    "Now that we have the necessary components (Table Embeddings, Value Embeddings, Few-Shot Embeddings, we can build out the entire pipeline.\n",
    "\n",
    "First we declare the evaluation dataset through the ```eval_df``` variable, and the ```queries``` variable to store all the Natural Language Queries (NLQs). Please note that if you wish to use a different evaluation dataset, you will need to change the code for ```eval_df``` and ```queries``` variables accordingly. All query results are stored in the ```df``` variable.\n",
    "\n",
    "Furthermore, you will need to provide your Huggingface token under the ```TOKEN``` variable, as well as the Google API Key under the ```API_KEY``` variable. This is automatically provided through loading all tokens noted in the ```config.yaml```. Lastly, you will need to alter the ```iterations``` variable, depending on how many times you wish to run through the evaluation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18a944-5e48-4740-afec-d8a81d5dad2e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_json('data/evaluation/Defender_Evaluation.jsonl', lines=True)\n",
    "queries= list(eval_df['context'])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "query_count = 0\n",
    "\n",
    "client = genai_client.Client(api_key=API_KEY)\n",
    "iterations = 5\n",
    "\n",
    "with open('data/DataCatalogs/Defender_DataCatalog.yml', 'r') as file:\n",
    "    defender_catalog = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d895e-a7c7-40ba-9fd8-a650b32035b5",
   "metadata": {},
   "source": [
    "The following snippet is used to store all intermediary responses from the SLM, final responses from the LLM Oracle, as well as all latency measures from the SLM and LLM Oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe22239-e954-46f4-bf95-9489b7209c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []\n",
    "all_nl2kql_txts = []\n",
    "final_responses = []\n",
    "slm_response_times = []\n",
    "oracle_queries = []\n",
    "oracle_response_times = []\n",
    "\n",
    "oracle_token_count = 0\n",
    "count = 0\n",
    "\n",
    "model_name = ''\n",
    "model_args = {\n",
    "    \"token\": TOKEN,\n",
    "    \"dtype\": torch.float32,\n",
    "    \"device_map\": \"auto\",\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **model_args).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=TOKEN)\n",
    "pipe = transformers.pipeline(\"text-generation\",\n",
    "                             model=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             dtype=model_args[\"dtype\"],\n",
    "                             temperature=0.2,\n",
    "                             do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f059e-d474-46d9-b963-df1dff9db407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_inline_kql(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = re.search(\n",
    "        r\"```(?:kusto|kql)\\s*([\\s\\S]*?)```\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return None\n",
    "\n",
    "    \n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "def extract_kql_from_result(result_text: str, prompt_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip the echoed prompt and return the FIRST ```kusto ...``` block.\n",
    "    If no fenced block is found, return the remaining text.\n",
    "    \"\"\"\n",
    "    full_norm = norm(result_text)\n",
    "    prompt_norm = norm(prompt_text)\n",
    "\n",
    "    # Strip prompt prefix if it was echoed\n",
    "    if full_norm.startswith(prompt_norm):\n",
    "        after_prompt = full_norm[len(prompt_norm):]\n",
    "    else:\n",
    "        header = \"# KQL Generation Instructions\"\n",
    "        idx = full_norm.find(header)\n",
    "        if idx != -1:\n",
    "            after_prompt = full_norm[idx + len(header):]\n",
    "        else:\n",
    "            after_prompt = full_norm\n",
    "\n",
    "    # Extract FIRST ```kusto ... ``` block\n",
    "    m = re.search(r\"```kusto\\s*(.*?)```\", after_prompt,\n",
    "                  flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    else:\n",
    "        return after_prompt.strip()\n",
    "\n",
    "\n",
    "for i in range(0, iterations):\n",
    "        \n",
    "    for query_prompt in queries:\n",
    "    \n",
    "        responses = []\n",
    "        query_response = get_query_embedding(query_prompt)\n",
    "        \n",
    "        # Get the relevant tables to the query:\n",
    "        defender_embedding_vals = list(table_embeddings.values())\n",
    "        cosine_similarities = [cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry).reshape(1,-1)) for entry in defender_embedding_vals]\n",
    "        \n",
    "        cosine_similarities_vals = [float(entry) for entry in cosine_similarities]\n",
    "        top_5_idx = np.argsort(cosine_similarities_vals)[-5:]\n",
    "        table_lst = list(table_embeddings.keys())\n",
    "        \n",
    "        filtered_tables = []\n",
    "        for idx in top_5_idx:\n",
    "            filtered_tables.append(table_lst[idx])\n",
    "        \n",
    "        relevant_columns = dict()\n",
    "        \n",
    "        for k in filtered_tables:\n",
    "            filter_lst = [entry for entry in list(value_embeddings.keys()) if k in entry]\n",
    "            sub_dict = {key: value_embeddings[key] for key in filter_lst}\n",
    "            cosine_similarities = []\n",
    "            \n",
    "            for key in sub_dict:\n",
    "                cosine_similarity_val = cosine_similarity(np.array(value_embeddings[key]).reshape(1,-1), \n",
    "                                                          np.array(query_response).reshape(1,-1))\n",
    "                cosine_similarities.append(cosine_similarity_val[0].item())\n",
    "        \n",
    "            top_5_cols_idx = np.argsort(cosine_similarities)[-5:]\n",
    "        \n",
    "            col_lst = list(sub_dict.keys())\n",
    "            relevant_cols = []\n",
    "            for idx in top_5_cols_idx:\n",
    "                relevant_columns[col_lst[idx]] = cosine_similarities[idx]\n",
    "        \n",
    "        # Top 5 Values:\n",
    "        final_vals = list(dict(sorted(relevant_columns.items(), key=lambda item: item[1], reverse=True)).keys())[0:5]\n",
    "        final_vals_revised = []\n",
    "        for entry in final_vals:\n",
    "            try:\n",
    "                final_vals_revised.append(re.search(r'Value Name:(.*)', entry).group(1))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        # Filter Few-Shot by Top t tables:\n",
    "        filtered_fsdb = [entry for entry in fsdb if len(set(entry['tables']).intersection(set(filtered_tables))) > 0]\n",
    "        \n",
    "        # Semantic Similarity Matching:\n",
    "        # nlq, f = 2\n",
    "        \n",
    "        fsdb_embeddings = []\n",
    "        fsdb_count = 0\n",
    "        for entry in filtered_fsdb:\n",
    "            fsdb_response = client.models.embed_content(model = 'text-embedding-004',\n",
    "                                                        contents = f\"{entry['nlq']}\")\n",
    "        \n",
    "            fsdb_embeddings.append({'NLQ': entry['nlq'], 'Embedding': fsdb_response.embeddings[0].values, 'KQL': entry['kql']})\n",
    "            fsdb_count = fsdb_count + 1\n",
    "            time.sleep(2)\n",
    "            \n",
    "        cosine_similarities_fsdb = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': float(cosine_similarity(np.array(query_response).reshape(1,-1), np.array(entry['Embedding']).reshape(1,-1)))} for entry in fsdb_embeddings]\n",
    "        \n",
    "        # Sort and find the Top 2 NLQ Entries:\n",
    "        cosine_similarities_fsdb_sorted = sorted(cosine_similarities_fsdb, key=lambda x: x['Similarity'], reverse=True)[0:2]\n",
    "        \n",
    "        # Filter KQL Queries:\n",
    "        cosine_similarities_fsdb_sorted = [{'NLQ': entry['NLQ'], 'KQL': entry['KQL'], 'Similarity': entry['Similarity']} for entry in cosine_similarities_fsdb_sorted]\n",
    "        \n",
    "        SCHEMA_PLACEHOLDER = \"\"\n",
    "        EXAMPLES_PLACEHOLDER = \"\"\n",
    "        USER_PLACEHOLDER = f\"{query_prompt}\"\n",
    "        \n",
    "        with open(prompt_template_path, 'r') as f:\n",
    "            txt = f.read()\n",
    "        \n",
    "        # Add Table and Schema Information:\n",
    "        for k in filtered_tables:\n",
    "            temp_col_lst = []\n",
    "            table = k\n",
    "            \n",
    "            for entry in defender_catalog:\n",
    "                if entry['Name'] == table:\n",
    "                    temp_col_lst = [subentry['Name'] for subentry in entry['Columns']]\n",
    "                    \n",
    "            #cols = relevant_columns[key]\n",
    "            col_combined = \", \".join(temp_col_lst)\n",
    "            SCHEMA_PLACEHOLDER += f\"Table: {k}, Columns: {col_combined}\"\n",
    "            SCHEMA_PLACEHOLDER += \"\\n\"\n",
    "        \n",
    "        # SCHEMA_PLACEHOLDER = \"Tables: \" + \", \".join(filtered_tables)\n",
    "        txt = txt.replace('{{SCHEMA_PLACEHOLDER}}', SCHEMA_PLACEHOLDER)\n",
    "        txt = txt.replace('{{USER_REQUEST_PLACEHOLDER}}', USER_PLACEHOLDER)\n",
    "        \n",
    "        # Add Examples:\n",
    "        for entry in cosine_similarities_fsdb_sorted:\n",
    "            EXAMPLES_PLACEHOLDER += f\"NLQ: {entry['NLQ']} \\n + KQL: {entry['KQL']}\"\n",
    "            EXAMPLES_PLACEHOLDER += \"\\n\"\n",
    "            \n",
    "        txt = txt.replace('{{EXAMPLES_PLACEHOLDER}}', EXAMPLES_PLACEHOLDER)\n",
    "    \n",
    "        # Declare the SLMs:\n",
    "        llmGeneratorOne = RunnableLambda(HuggingFacePipeline(pipeline=pipe))\n",
    "    \n",
    "        # Declare Langchain Runnable for Parallelism:\n",
    "        runnable = RunnableParallel(result_one = llmGeneratorOne)\n",
    "        \n",
    "        # Perform the query:\n",
    "        start = time.time()\n",
    "        results = runnable.invoke(txt)\n",
    "        end = time.time()\n",
    "    \n",
    "        slm_response_times.append(end-start)\n",
    "    \n",
    "        # Add main query:\n",
    "        all_nl2kql_txts.append(txt)\n",
    "        \n",
    "        # Add all responses:\n",
    "        # responses.append(re.search(r'model\\\\n(.*)', results['result_one'], flags=re.DOTALL).group(1))\n",
    "        responses = []\n",
    "        for key in [\"result_one\"]:\n",
    "            full = results[key]\n",
    "            kql_query = extract_kql_from_result(full, txt)\n",
    "            responses.append(kql_query)\n",
    "\n",
    "        # Store and debug-print\n",
    "        all_responses.append(responses)\n",
    "        for q in responses:\n",
    "            print(q)      \n",
    "        \n",
    "        # all_responses.append(responses)\n",
    "        # print(all_responses)\n",
    "\n",
    "        #  Make changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s). You may use the following tables and columns:\\n\\n{schemas[0]}\\n\\n.\n",
    "\n",
    "\n",
    "        try: \n",
    "            # Oracle Input can be determined:\n",
    "            oracle_prompt = f\"Given a Natural Language Query and a list of KQL queries, determine which of the following KQL queries is most syntactically and semantically correct:\\n\\nNatural Language Query: {query_prompt}\\nResponses: {', '.join(responses)}.\\n\\nMake changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s).\"           \n",
    "            # f\"Given a Natural Language Query and a list of KQL queries, determine which of the following KQL queries is most syntactically and semantically correct:\\n\\nNatural Language Query: {query_prompt}\\nResponses: {', '.join(responses)}.\\n\\nMake changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s). You may use the following tables and columns: {SCHEMA_PLACEHOLDER}\\n\\nReturn the correct answer without explanation.\"\n",
    "            oracle_start = time.time()\n",
    "            oracle_response = client.models.generate_content(model = 'models/gemini-2.0-flash', contents = oracle_prompt)\n",
    "            oracle_end = time.time()\n",
    "            oracle_response_times.append(oracle_end - oracle_start)\n",
    "        \n",
    "            # Make changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s). You may use the following tables and columns:\\n\\n{schemas[0]}\\n\\n.\n",
    "            oracle_queries.append(oracle_prompt)\n",
    "            \n",
    "            # Oracle Output can be determined:\n",
    "            print(oracle_response.candidates[0].content.parts[0].text)\n",
    "            final_responses.append(oracle_response.candidates[0].content.parts[0].text)\n",
    "            count += 1\n",
    "            print(count)\n",
    "        except:\n",
    "            time.sleep(60)\n",
    "            oracle_prompt = f\"Given a Natural Language Query and a list of KQL queries, determine which of the following KQL queries is most syntactically and semantically correct:\\n\\nNatural Language Query: {query_prompt}\\nResponses: {', '.join(responses)}.\\n\\nMake changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s).\"\n",
    "            # f\"Given a Natural Language Query and a list of KQL queries, determine which of the following KQL queries is most syntactically and semantically correct:\\n\\nNatural Language Query: {query_prompt}\\nResponses: {', '.join(responses)}.\\n\\nMake changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s). You may use the following tables and columns: {SCHEMA_PLACEHOLDER}\\n\\nReturn the correct answer without explanation.\"            \n",
    "            oracle_start = time.time()\n",
    "            oracle_response = client.models.generate_content(model = 'models/gemini-2.0-flash', contents = oracle_prompt)\n",
    "            oracle_end = time.time()\n",
    "            oracle_response_times.append(oracle_end - oracle_start)\n",
    "        \n",
    "            # Make changes as necessary to refine the best KQL query, and ensure that each column in the KQL query belongs to its respective table(s). You may use the following tables and columns:\\n\\n{schemas[0]}\\n\\n.\n",
    "            oracle_queries.append(oracle_prompt)\n",
    "            \n",
    "            # Oracle Output can be determined:\n",
    "            print(oracle_response.candidates[0].content.parts[0].text)\n",
    "            final_responses.append(oracle_response.candidates[0].content.parts[0].text)\n",
    "            count += 1\n",
    "            print(count)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89916f-3750-4bf9-b9c6-183c45dbfcfe",
   "metadata": {},
   "source": [
    "### Step #6: Cost Analysis\n",
    "\n",
    "An important aspect that we are tracking with these experiments is the amount of money it costs to run both SLMs and LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61690fd4-e80b-4bff-adc4-16ec6dee63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = []\n",
    "for i in range(0, iterations):\n",
    "    all_queries = all_queries + queries\n",
    "\n",
    "df = pd.DataFrame(columns = ['NLQ', 'LLM-KQL'])\n",
    "df['NLQ'] = all_queries\n",
    "df['LLM-KQL'] = final_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb10c9-443d-4a9a-a1b5-d437f8d69332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "HF_TOKEN = token_config['huggingface']['token']\n",
    "SLM_MODEL_ID = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "\n",
    "slm_client = InferenceClient(SLM_MODEL_ID, token=HF_TOKEN)\n",
    "slm_tokenizer = AutoTokenizer.from_pretrained(SLM_MODEL_ID)\n",
    "def slm_count_tokens(text: str) -> int:\n",
    "    # add_special_tokens=False is usually closer to how APIs bill\n",
    "    return len(slm_tokenizer.encode(text, add_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bb644-b93d-43d2-b2de-93cf179ab29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slm_input_cost_per_million = 0.15\n",
    "slm_output_cost_per_million = 0.15\n",
    "oracle_input_cost_per_million = 0.10\n",
    "oracle_output_cost_per_million = 0.40\n",
    "\n",
    "total_slm_input_tokens = 0\n",
    "total_slm_output_tokens = 0\n",
    "total_oracle_input_tokens = 0\n",
    "total_oracle_output_tokens = 0\n",
    "\n",
    "# Calculate SLM Input tokens:\n",
    "for entry in all_nl2kql_txts:\n",
    "    total_slm_input_tokens +=slm_count_tokens(entry)\n",
    "\n",
    "total_slm_input_tokens = total_slm_input_tokens\n",
    "print(\"Finished calculating SLM input tokens\")\n",
    "\n",
    "# Calculate SLM Output tokens:\n",
    "for sublst in all_responses:\n",
    "    for resp in sublst:\n",
    "        total_slm_output_tokens +=slm_count_tokens(resp)\n",
    "print(\"Finished calculating SLM output tokens\")\n",
    "\n",
    "# Calculate Oracle input tokens:\n",
    "for entry in oracle_queries:\n",
    "    total_oracle_input_tokens = total_oracle_input_tokens + client.models.count_tokens(model=\"gemini-2.0-flash\", contents=entry).total_tokens\n",
    "print(\"Finished calculating Oracle input tokens\")\n",
    "\n",
    "# Calculate Oracle output tokens:\n",
    "for result in final_responses:\n",
    "    total_oracle_output_tokens = total_oracle_output_tokens + client.models.count_tokens(model=\"gemini-2.0-flash\", contents=result).total_tokens\n",
    "print(\"Finished calculating Oracle output tokens\")\n",
    "\n",
    "total_costs = ((slm_input_cost_per_million * total_slm_input_tokens)/1000000) + ((slm_output_cost_per_million * total_slm_output_tokens)/1000000) + ((oracle_input_cost_per_million * total_oracle_input_tokens)/1000000) + ((oracle_output_cost_per_million * total_oracle_output_tokens)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbe96b-09c1-41d9-a1a9-84456f44258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š SLM (Small Language Model)\")\n",
    "print(f\"  Input tokens:  {total_slm_input_tokens:,}\")\n",
    "print(f\"  Output tokens: {total_slm_output_tokens:,}\")\n",
    "print(f\"  Total tokens:  {total_slm_input_tokens + total_slm_output_tokens:,}\")\n",
    "\n",
    "print(\"\\nðŸ”® Oracle (Gemini 2.0 Flash)\")\n",
    "print(f\"  Input tokens:  {total_oracle_input_tokens:,}\")\n",
    "print(f\"  Output tokens: {total_oracle_output_tokens:,}\")\n",
    "print(f\"  Total tokens:  {total_oracle_input_tokens + total_oracle_output_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10444fb3-04c5-4d12-a81f-319a46f4a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Cost: ${round(total_costs/5, 3)}\")\n",
    "print(f\"Average Latency (in sec): {round((sum(slm_response_times)/len(slm_response_times)) + (sum(oracle_response_times)/len(oracle_response_times)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1b7ed-aa3e-4534-b203-7671d1c34108",
   "metadata": {},
   "source": [
    "### Step #7: Saving all Outputs\n",
    "\n",
    "To keep track of all inputs and outputs utilized through the pipeline, we save the results from all variables in pickle files. This allows us to calculate more valuable information (i.e. Latency, Tokens used per query, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6794144-78a1-443e-8755-916a192c2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path as needed (i.e. Where the results will be stored):\n",
    "base_path = \"saleha-general-refinement-1\"\n",
    "\n",
    "with open(f\"{base_path}all_responses.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_responses, f)\n",
    "\n",
    "with open(f\"{base_path}all_nl2kql_txts.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_nl2kql_txts, f)\n",
    "\n",
    "with open(f\"{base_path}final_responses.pkl\", 'wb') as f:\n",
    "    pickle.dump(final_responses, f)\n",
    "\n",
    "with open(f\"{base_path}oracle_queries.pkl\", 'wb') as f:\n",
    "    pickle.dump(oracle_queries, f)\n",
    "\n",
    "with open(f\"{base_path}slm_response_times.pkl\", 'wb') as f:\n",
    "    pickle.dump(slm_response_times, f)\n",
    "\n",
    "with open(f\"{base_path}oracle_response_times.pkl\", 'wb') as f:\n",
    "    pickle.dump(oracle_response_times, f)\n",
    "\n",
    "# Save the main results, change the file path as needed (and RQ as needed):\n",
    "df.to_csv(f\"{base_path}rq5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b3e9e-cca6-404d-8061-8f37d649f74c",
   "metadata": {},
   "source": [
    "### Step #8: Latency Measures:\n",
    "\n",
    "Using the latency measures that we calculated from running the pipeline, we can calculate the total latency of our pipeline based on inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f476b-beb2-4303-8563-eea32699a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_latency = np.array(slm_response_times) + np.array(oracle_response_times)\n",
    "total_latency = total_latency.tolist()\n",
    "print(sum(total_latency)/len(total_latency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736bf5d-a959-42a7-9680-0861dfdd493e",
   "metadata": {},
   "source": [
    "### Step #9: Export to YAML:\n",
    "\n",
    "Next, we clean the results and export all of them to a .yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcda3a9-819a-49ff-b003-47f3c9176f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_df.columns)\n",
    "baseline = []\n",
    "lst = []\n",
    "\n",
    "# Change this string as needed (needs to be a .yaml):\n",
    "path_to_file_export = \"\"\n",
    "\n",
    "for i in range(0, iterations):\n",
    "    baseline = baseline + list(eval_df['baseline']) \n",
    "df['baseline'] = baseline\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        lst.append(re.search(r'```(?:kusto|kql)(.*)```', row['LLM-KQL'], flags=re.DOTALL).group(1))\n",
    "    except:\n",
    "        print(idx)\n",
    "        lst.append(row['LLM-KQL'])\n",
    "\n",
    "df['LLM-KQL-Extracted'] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce79579-bbc6-45ef-997e-f8b1f70a86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for general refinement deepseek \n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# def extract_inline_kql(text: str):\n",
    "#     if not isinstance(text, str):\n",
    "#         return None\n",
    "#     m = re.search(\n",
    "#         r\"```(?:kusto|kql)\\s*([\\s\\S]*?)```\",\n",
    "#         text,\n",
    "#         flags=re.IGNORECASE\n",
    "#     )\n",
    "#     if m:\n",
    "#         return m.group(1).strip()\n",
    "#     return None\n",
    "\n",
    "\n",
    "# print(eval_df.columns)\n",
    "# baseline = []\n",
    "\n",
    "# # Change this string as needed (needs to be a .yaml):\n",
    "# path_to_file_export = \"saleha-general-refinement-1\"\n",
    "\n",
    "# # Repeat baseline for each iteration\n",
    "# for i in range(0, iterations):\n",
    "#     baseline = baseline + list(eval_df['baseline'])\n",
    "# df['baseline'] = baseline\n",
    "\n",
    "# # 1) apply extractor to get just the fenced KQL (or None)\n",
    "# df[\"LLM_KQL_query_only\"] = df[\"LLM-KQL\"].apply(extract_inline_kql)\n",
    "\n",
    "# # 2) if extractor failed (None/NaN), fall back to raw LLM-KQL\n",
    "# df[\"LLM-KQL-Extracted\"] = df[\"LLM_KQL_query_only\"].where(\n",
    "#     df[\"LLM_KQL_query_only\"].notna(),  # condition\n",
    "#     df[\"LLM-KQL\"]                      # fallback\n",
    "# )\n",
    "\n",
    "# # 3) final safety: ensure everything is a string (no None in YAML)\n",
    "# df[\"LLM-KQL-Extracted\"] = df[\"LLM-KQL-Extracted\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c2e5-9e0c-4e3e-8fee-9f269cbeb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"queries\": []}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    results[\"queries\"].append({\n",
    "         \"id\": idx,\n",
    "         \"prompt\": row[\"NLQ\"],\n",
    "         \"connector\": \"Defender\",\n",
    "         \"baseline\": row[\"baseline\"].strip(),\n",
    "         \"llmResult\": row['LLM-KQL-Extracted']\n",
    "})\n",
    "\n",
    "with open(path_to_file_export, 'w') as f:\n",
    "    yaml.dump(results, \n",
    "              f, \n",
    "              sort_keys=False,\n",
    "              default_style='|',\n",
    "              allow_unicode=True,\n",
    "              width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc2236-0408-4e37-bbb0-6fd85cf02745",
   "metadata": {},
   "source": [
    "### Step #10: Metrics\n",
    "\n",
    "The following snippets will obtain all metrics for the results that you have just calculated. Please note that full path **must** be specified for ```file_of_interest``` and ```folder```. If you need help specifying the full path, use the ```!pwd``` command in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5d33c-95bc-4f8d-a199-9ca522970804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE FOLLOWING VARIABLE:\n",
    "runner = \"../offline_metrics_pipeline/offline-metrics-pipeline/offline_metrics_runner.py\"\n",
    "\n",
    "# CHANGE THE FOLLOWING VARIABLES:\n",
    "# This should point to where your .yaml is currently stored (entire path must be specified):\n",
    "file_of_interest = \"\"\n",
    "\n",
    "# These should point to where you would like to store your results (entire path must be specified):\n",
    "folder = \"\"\n",
    "results_file = \"testing.csv\"\n",
    "\n",
    "!python {runner} {file_of_interest} {folder} {results_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
