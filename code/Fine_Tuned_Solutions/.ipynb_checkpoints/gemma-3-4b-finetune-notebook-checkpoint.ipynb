{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dd5d07-ccf2-43b6-99b5-0c2cf04bff85",
   "metadata": {},
   "source": [
    "### Fine Tuning Gemma-3-4B using Low Rank Adaption (LoRA):\n",
    "\n",
    "The following notebook fine tunes an SLM on Kusto Query Language (KQL) queries. We use a ground truth dataset separate from the NL2KQL evaluation to fine-tune an LLM on KQL queries. There are two different types of fine-tuning that we try:\n",
    "\n",
    "- Supervised Fine-Tuning\n",
    "- CoT Fine-Tuning\n",
    "\n",
    "The following notebook will walk you through both processes and explain how to fine-tune a Gemma-3-4B-IT SLM on both types of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acb677-9c60-4c8c-b9bf-bc24296685ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Huggingface Imports:\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import login\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, PeftModel, get_peft_model #(Performance Enhancing Fine-Tuning)\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    token_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7ccdc-6b71-49bd-886f-4b93955d4a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKEN = token_config['huggingface']['token']\n",
    "\n",
    "# Two Options: \"Supervised\", or \"CoT\" (for Chain-of-Thought Fine-Tuning)\n",
    "mode = \"CoT\"\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d6adb-a977-49c8-b068-44c5cd94d082",
   "metadata": {},
   "source": [
    "**NOTE:** You may run into some issues in Fine-Tuning models if you do not login formally through Huggingface. For this reason, run the following cell and put the Huggingface Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbed9e-0ec9-465d-b74d-d8282bb48dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5013a54-db9a-440b-a385-7fe5f753ab0b",
   "metadata": {},
   "source": [
    "### Defining Default Variables:\n",
    "\n",
    "First we define the base model, the dataset that we want to fine-tune the base LLM on, and the new model name to save the fine-tuned model to. For the purposes of this fine-tuning exercise, we do not use 4-bit quantization because we have sufficient resources to fine-tune the LLM. In the case that resources are limited, feel free to incorporate 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3b988-4a10-4957-ab89-e4ced08e74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace paths ONLY:\n",
    "\n",
    "base_model = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e99f0-a2f0-47a5-9a9e-9d8de96bbbef",
   "metadata": {},
   "source": [
    "### Preprocessing the Dataset:\n",
    "\n",
    "We will need to preprocess the original ground truth dataset into an instruction format that is similar to the NL2KQL format. The purpose behind doing this is to make sure the structures remain consistent during training and inference time. This ensures a fair chance is given to the model in order to learn the same structure prompt as NL2KQL.\n",
    "\n",
    "Because we do not know the respective schemas, values, and examples chosen by NL2KQL in advance, we will forgo these features when altering the structure of the prompts in Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9d134-c4e1-4512-89e0-4c6de394eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training_data.csv\")\n",
    "df = df[0:1000]\n",
    "\n",
    "lst = []\n",
    "for idx, row in df.iterrows():\n",
    "    lst.append(f\"{row['Explanation']}. Therefore, the answer is ```kusto\\n{row['kql']}```\")\n",
    "\n",
    "df['kql_formatted'] = lst\n",
    "df = df[['tables', 'theme', 'nlq', 'kql',\n",
    "       'similarity_score', 'syntax', 'semantic', 'Explanation', 'Description',\n",
    "       'kql_formatted']]\n",
    "\n",
    "df.to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabd3f0-4b66-4d84-9874-c3a86a66e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Message:\n",
    "user_message = \"\"\n",
    "with open(\"deepseek-prompt-good.txt\", \"r\") as f:\n",
    "    user_message = f.read()\n",
    "    \n",
    "def create_conversation(sample):\n",
    "    if mode == \"Supervised\":\n",
    "        return {\"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message.format(USER_REQUEST_PLACEHOLDER = sample['nlq'])},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"kql\"]}\n",
    "            ]\n",
    "          }\n",
    "    elif mode == \"CoT\":\n",
    "        return {\"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message.format(USER_REQUEST_PLACEHOLDER = sample['nlq'])},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"kql_formatted\"]}\n",
    "            ]\n",
    "          }\n",
    "    \n",
    "dataset = load_dataset(\"csv\", data_files = \"training_data.csv\", split = \"train\")\n",
    "\n",
    "# Convert dataset to OAI messages:\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "\n",
    "# Split Dataset into 800 training samples and 200 test samples:\n",
    "dataset = dataset.train_test_split(test_size=200/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d57984-4865-438d-9622-92a552fcc2c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha_ranges = [2, 4, 8]\n",
    "rank_ranges = [1, 2, 4]\n",
    "lora_dropout = [0.1, 0.2]\n",
    "\n",
    "for alpha in alpha_ranges:\n",
    "    for rank in rank_ranges:\n",
    "        for dropout in lora_dropout:\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(base_model, \n",
    "                                             token=TOKEN, \n",
    "                                             torch_dtype=\"auto\", device_map={\"\": 0})\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_model, token=TOKEN)\n",
    "\n",
    "            if mode == \"Supervised\":\n",
    "                new_model = f\"Supervised_GemmaFineTune_{alpha}_{rank}_{dropout}\"\n",
    "            elif mode == \"CoT\":\n",
    "                new_model = f\"CoT_GemmaFineTune_{alpha}_{rank}_{dropout}\"\n",
    "            \n",
    "            # Low Rank Adaption (Fine-Tuning Method):\n",
    "            peft_parameters = LoraConfig(\n",
    "                lora_alpha = alpha,\n",
    "                lora_dropout = dropout, \n",
    "                r = rank, \n",
    "                bias=\"none\", \n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                target_modules=[\"q_proj\", \"k_proj\"]\n",
    "            )\n",
    "\n",
    "            # Incorporate the LoRA parameters into the model:\n",
    "            model = get_peft_model(model, peft_parameters)\n",
    "\n",
    "            trainer = SFTTrainer(\n",
    "                model = model,\n",
    "                train_dataset = dataset[\"train\"],\n",
    "                eval_dataset = dataset['test'],\n",
    "                peft_config=peft_parameters,\n",
    "                processing_class = tokenizer,\n",
    "                args = SFTConfig(\n",
    "                    num_train_epochs = 1,\n",
    "                    per_device_train_batch_size=5,\n",
    "                    gradient_accumulation_steps=1,\n",
    "                    optim=\"paged_adamw_32bit\",\n",
    "                    remove_unused_columns=False,\n",
    "                    save_steps=25,\n",
    "                    logging_steps=25,\n",
    "                    learning_rate=2e-4,\n",
    "                    weight_decay=0.001,\n",
    "                    fp16=False,\n",
    "                    bf16=False,\n",
    "                    max_grad_norm=0.3,\n",
    "                    max_steps=-1,\n",
    "                    warmup_ratio=0.03,\n",
    "                    group_by_length=True,\n",
    "                    lr_scheduler_type=\"constant\"),\n",
    "            )\n",
    "\n",
    "            # Train the model:\n",
    "            trainer.train()\n",
    "\n",
    "            # Final Loss:\n",
    "            avg_train_loss = trainer.state.log_history[-1]['train_loss']\n",
    "\n",
    "            # Evaluate the model:\n",
    "            metrics = trainer.evaluate()\n",
    "            metrics['avg_train_loss'] = avg_train_loss\n",
    "\n",
    "            # Save the models:\n",
    "            trainer.model.save_pretrained(new_model)\n",
    "            trainer.tokenizer.save_pretrained(new_model)\n",
    "\n",
    "            # Output metrics to .pkl\n",
    "            with open(f\"{new_model}/metrics.pkl\", \"wb\") as f:\n",
    "                pickle.dump(metrics, f)\n",
    "\n",
    "            model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f49a4-3d48-42cb-9a31-91d636b693f3",
   "metadata": {},
   "source": [
    "### Choosing the Best Model:\n",
    "\n",
    "From here, we can choose the best fine-tuned model based on the assessed evaluation loss. In each of the models reproduced, we stored a ```metrics.pkl``` file to keep track of the evaluation loss. To proceed forward, do the following:\n",
    "\n",
    "1. Create two folders: ```Supervised_Fine_Tuning``` and ```CoT_Fine_Tuning```\n",
    "2. Move each fine-tuned model into the respective folders, and change the ```folder``` variable to either ```Supervised_Fine_Tuning``` or ```CoT_Fine_Tuning```.\n",
    "3. Run the snippet below to produce the Fine-Tuned Models\n",
    "4. Run the final block below to get the ```alpha```, ```rank```, and ```LoRA Dropout``` value that minimizes evaluation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba22d84-d9d0-4f24-aa58-62857b744044",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7574d-6908-46c0-a921-6fcbb5bd12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_folders = [entry for entry in os.listdir(main_folder)]\n",
    "\n",
    "eval_loss = np.inf\n",
    "alpha_best = -1\n",
    "rank_best = -1\n",
    "dropout_best = -1\n",
    "best_folder = None\n",
    "\n",
    "for folder in sorted(gemma_folders):\n",
    "\n",
    "    if mode == \"Supervised\":\n",
    "        regex = r'Supervised_GemmaFineTune_(.*)_(.*)_(.*)'\n",
    "    elif mode == \"CoT\":\n",
    "        regex = r'CoT_GemmaFineTune_(.*)_(.*)_(.*)'\n",
    "        \n",
    "    with open(f\"{main_folder}/{folder}/metrics.pkl\", \"rb\") as f:\n",
    "        metrics_file = pickle.load(f)\n",
    "\n",
    "    match = re.search(regex, folder, flags=re.DOTALL)\n",
    "    alpha = match.group(1)\n",
    "    rank = match.group(2)\n",
    "    dropout = match.group(3)\n",
    "\n",
    "    if metrics_file[\"eval_loss\"] < eval_loss:\n",
    "        eval_loss = metrics_file[\"eval_loss\"]\n",
    "        alpha_best = alpha\n",
    "        rank_best = rank\n",
    "        dropout_best = dropout\n",
    "        best_folder = folder\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(f\"  folder:      {best_folder}\")\n",
    "print(f\"  eval_loss:   {eval_loss}\")\n",
    "print(f\"  alpha_best:  {alpha_best}\")\n",
    "print(f\"  rank_best:   {rank_best}\")\n",
    "print(f\"  dropout_best:{dropout_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de12e3-cb3b-4071-823a-c07b43c67b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gemma_folders = [entry for entry in os.listdir(main_folder)]\n",
    "all_metrics = []\n",
    "\n",
    "for folder in sorted(gemma_folders):\n",
    "    if mode == \"Supervised\":\n",
    "        regex = r'Supervised_GemmaFineTune_(.*)_(.*)_(.*)'\n",
    "    elif mode == \"CoT\":\n",
    "        regex = r'CoT_GemmaFineTune_(.*)_(.*)_(.*)'\n",
    "        \n",
    "    try:\n",
    "        with open(f\"{main_folder}/{folder}/metrics.pkl\", \"rb\") as f:\n",
    "            metrics_file = pickle.load(f)\n",
    "        \n",
    "        match = re.search(regex, folder, flags=re.DOTALL)\n",
    "        alpha = match.group(1)\n",
    "        rank = match.group(2)\n",
    "        dropout = match.group(3)\n",
    "        \n",
    "        # Store all metrics\n",
    "        all_metrics.append({\n",
    "            'folder': folder,\n",
    "            'eval_loss': metrics_file[\"eval_loss\"],\n",
    "            'alpha': alpha,\n",
    "            'rank': rank,\n",
    "            'dropout': dropout\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {folder}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for easy viewing and analysis\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Sort by eval_loss to see best models first\n",
    "metrics_df = metrics_df.sort_values('eval_loss')\n",
    "\n",
    "print(\"All models:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Still get the best model\n",
    "best_model = metrics_df.iloc[0]\n",
    "print(\"\\nBest model:\")\n",
    "print(f\"  folder:      {best_model['folder']}\")\n",
    "print(f\"  eval_loss:   {best_model['eval_loss']}\")\n",
    "print(f\"  alpha_best:  {best_model['alpha']}\")\n",
    "print(f\"  rank_best:   {best_model['rank']}\")\n",
    "print(f\"  dropout_best:{best_model['dropout']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
