\section{Evaluation}
\label{s:eval}
% \wajih{Each RQ just need to have three paragraphs.
%  Paragraph 1: Motivation \& Setup
% Restate the RQ clearly.
% Explain why it matters.
% Describe the specific experimental setup (models, prompts, datasets, configs).
% Paragraph 2: Results Presentation
% Present key quantitative results.
% Refer to the relevant table or figure.
% Highlight main trends briefly.
% Paragraph 3: Key Findings / Interpretation
% Explain what the results mean.
% Discuss why trends occur.}

We evaluate the effectiveness of our proposed design through a series of experiments conducted on a machine with an AMD EPYC 9534 64-Core Processor, an NVIDIA H100 NVL GPU, and Ubuntu 22.04.5 LTS. We address seven research questions (RQs) as follows. For all RQs, we access and query SLMs using the Huggingface Python package \cite{Huggingface}, which avoids input/output token rate limits. For RQ5 and RQ6, we query Google Gemini 2.0 Flash via the Google GenAI package \cite{PyPI}. Our re-implemented NL2KQL pipeline (Section~\ref{sec:nl2kql-config}) is evaluated using Gemini 2.0 Flash, Phi-4, and GPT-4o.

\begin{itemize}[leftmargin=*]
\item \textbf{RQ1}: What is the baseline KQL reasoning ability of LLMs and SLMs without enhancements?
\item \textbf{RQ2}: How well does NL2KQL perform with SLMs?
\item \textbf{RQ3}: How do different prompting schemes affect NL2KQL with SLMs?
\item \textbf{RQ4}: How effective is LoRA fine-tuning for SLM-based KQL generation?
\item \textbf{RQ5}: Can a two-stage architecture with Oracle refinement match or surpass LLM performance at lower cost?
\item \textbf{RQ6}: How well does the two-stage architecture generalize to unseen schemas and tables?
\item \textbf{RQ7}: What are the ablation results for different components of the two-stage architecture?
\end{itemize}

Due to space constraints, we provide results for RQ6 and RQ7 in the Appendix~\ref{sec:general} and Appendix~\ref{sec:ablation}.


\begin{table}[!t]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
Model & Input Cost & Output Cost \\
\midrule
OpenAI GPT-5 & \$1.25 & \$10.00 \\
OpenAI GPT-4o & \$2.50 & \$10.00 \\
Microsoft Phi-4 (Microsoft) & \$0.125 & \$0.50 \\
Google Gemini 2.0 Flash (Google) & \$0.10 & \$0.40 \\
\midrule
Qwen-2.5-7B-Instruct-1M & \$0.30 & \$0.30 \\
Microsoft Phi-4-Mini (Microsoft)& \$0.075 & \$0.30 \\
Gemma-3-4B-IT (OpenRouter) & \$0.04 & \$0.08 \\
Gemma-3-1B-IT (OpenRouter) & \ Free & Free \\
\bottomrule
\end{tabular}
\caption{Costs per 1 million tokens of input and 1 million tokens of output from LLMs and SLMs}
\label{tab:cost-analysis}
\end{table}


\heading{Datasets} RQ1-RQ5 are evaluated using Microsoft's NL2KQL Defender Evaluation Dataset \cite{NL2KQL_Eval}, which contains 230 NLQ-KQL pairs referencing all tables in the NL2KQL Defender Schema. Instead of feeding NLQs directly to the models, we embed them in different prompting strategies and compare the generated queries against the baseline pairs to measure syntactic, semantic, and structural similarity. For RQ6, we obtained 83 NLQ-KQL pairs from the Microsoft Defender sections of Sentinel-Queries \cite{reprise99}, a publicly available GitHub repository. We used all available Defender queries from this repository and excluded queries designed for other data sources.
% \wajih{there are more queries on this link but you picked 83 why?. Reviewer will think that you are cherry picking if you don't mention criteria.}

\heading{Evaluation Metrics}
In order to evaluate these research questions and assess the effectiveness of our research methods, we 
first define the following evaluation metrics. These evaluation metrics have been previously defined
in NL2KQL \cite{nl2kql}, and we adapt them as
part of our evaluation process as well.

\PP{Syntax Score} evaluates whether an LLM-generated KQL query $\hat{q}$ is syntactically correct:
$\text{Syntax}(\hat{q}) = 1$ if $\hat{q}$ is syntactically correct, and $0$ otherwise.


\PP{Semantic Score} evaluates whether $\hat{q}$ is semantically correct within the schema $s$:
$\text{Semantic}(\hat{q}) = 1$ if $\hat{q}$ is semantically correct, and $0$ otherwise.

\PP{Table Score:} Evaluates the proportion of tables referenced in the $\hat{q}$ query $T(\hat{q})$ which are also references in q, but only if $T(q)$ is a subset of $T(\hat{q})$ otherwise, it’s zero.

\begin{equation}
  Table(q, \hat{q})=\begin{cases}
    \frac{\vert T(q) \cap T(\hat{q}) \vert}{\vert T(\hat{q}) \vert}, & \text{if $T(q) \subseteq T(\hat{q})$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

\PP{Filter Column Score:} Evaluates Jaccard similarity of the set of columns referenced in filters ($F_{col}(.)$) of $\hat{q}$ and $q$, where Jaccard of two sets is defined as $Jaccard(a, b) = \vert\frac{a \cap b}{a \cup b}\vert$

\PP{Filter Literal Score:} Evaluates Jaccard similarity of the set of literals used in filters of $\hat{q}$ and $q$.

In addition to adapting the metrics defined in NL2KQL, we also note two more metric measures:

\PP{Avg. Latency:} Average end-to-end model inference time per NLQ (s/query), measured from prompt assembly to final string, including network/API overhead. For the two-stage architecture, latency is the maximum per-NLQ end-to-end time across all SLMs plus the Oracle model's inference time.

\PP{Cost:} Total cost (USD) to process 230 NLQs from the NL2KQL Defender Evaluation Dataset, accounting for all input and output tokens per model.



\heading{Hyperparameters} To train SLMs using Parameter-Efficient Fine-Tuning (PEFT) and LoRA, we keep the following hyperparameters stable for Gemma-3-4B-IT in RQ4: one training epoch, batch size of 5 per device, learning rate of 0.0002, weight decay of 0.001, maximum gradient norm of 0.3, and warmup ratio of 0.03. The optimizer used is \texttt{paged\_adamw\_32bit}. This setup follows Google's fine-tuning guide for Gemma with Huggingface and QLoRA~\cite{Google_Fine_Tune}. For supervised fine-tuning, we vary the LoRA parameters and select the combination minimizing validation loss. Specifically, we test alpha ($\alpha$) values of 2, 4, and 8; rank ($r$) values of 1, 2, and 4; and dropout values of 0.1 and 0.2.



\heading{Embedding Model} An embedding model is an ML model that can be used to convert text into a numerical representation so that it can be conceptually
understood by LLMs or SLMs. The primary embedding model used in the following RQs is Google's text-embedding-004 model. The text-embedding-004
provides embedding queries for \$0.025 per 1 million input tokens, and \$0.02 per 1 million output tokens \cite{Google}. Furthermore,
text-embedding-004 is a lightweight, simplistic embedding model that has performed well across multiple numerous benchmarks and has strong multilingual
and domain-specific capabilities.


\heading{Cost Analysis} SLMs offer several advantages, including enhanced data security and lower response latency, but cost efficiency is one of their most significant benefits. Like LLMs, SLM costs are determined by input and output token counts, with per-million-token pricing varying by provider. For this analysis, we report prices from both model providers (e.g., Google, Microsoft) and third-party platforms (e.g., OpenRouter\cite{OpenRouter}) to illustrate an upper bound on costs across models. Table~\ref{tab:cost-analysis} presents the cost per 1M tokens for multiple LLMs and SLMs in USD. The cost gap is substantial: SLMs can offer up to 15x savings on input tokens alone compared to LLMs. When used effectively for code generation, SLMs can deliver significant performance at a fraction of traditional LLM costs.




\subsection{RQ1: Baseline LLM and SLM Performance}

% \Fix{Gemini is referred to as "Gemini 2.0 Flash" here, but earlier sections sometimes mention "Gemini 2.5 Flash." Consistency is needed.}

% \Fix{The analysis mixes metric results, error interpretation, and causal reasoning in one long block. There is no clear structure separating observations, comparisons, and insights, which makes the results hard to follow.}

In this research question, we determine the baseline KQL reasoning capability of LLMs and SLMs without any enhancements. This helps to understand how LLMs and SLMs independently perform in generating KQL queries. We perform experiments across four different LLMs: OpenAI's GPT-5 and GPT-4o, Google Gemini 2.0 Flash, and 
Microsoft Phi-4, and four different SLMs: Google Gemma-3-1B-IT, Google Gemma-3-4B-IT, Microsoft Phi-4-Mini-Instruct, and Qwen-2.5-7B-Instruct-1M. In these initial experiments, both LLMs and SLMs are not fed any contextual information needed to create the KQL query through the prompts. Rather, we exclusively
assess their inference abilities to produce KQL queries from a simple NLQ. We use the prompt as outlined in the 
Zero-Shot Prompting Strategy in Figure~\ref{fig:naive_zeroshot}. These results are shown in Table~\ref{tab:nl2kql-llm-slm-eval}.

% \begin{table*}[!t]
% \centering
% \begin{tabular}{lcccccccc}
% \toprule
% Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ & Avg. Latency & Total Cost (USD) \\
% \midrule
% NL2KQL main & 0.988 & 0.960 & 0.822 & 0.699 & 0.666 & -- & -- \\
% NL2KQL + Google Gemini 2.5 Flash & 0.922 & 0.787 & 0.678 & 0.478 & 0.517 & 11.109 & \$0.541 \\
% NL2KQL + Microsoft Phi-4 & 0.760 & 0.524 & 0.336 & 0.242 & 0.191 & 13.476 & \$0.132 \\
% \midrule
% Gemini 2.5 Flash (Zero-Shot) & 0.939 & 0.391 & 0.526 & 0.401 & 0.548 & 7.736 & \$0.044 \\
% Phi-4 (Zero-Shot) & 0.922 & 0.026 & 0.143 & 0.065 & 0.435 & 2.001 & \$0.010 \\
% \midrule
% Gemma 3-1B-IT (Zero-shot) & 0.504 & 0 & 0 & 0 & 0.317 & 0.825 & \$0.00 \\
% % Gemma 3-1B-IT (Few-shot) & 0.926 & 0.522 & 0.183 & 0.151 & 0.169 & 1.678 & \$0.00 \\
% Gemma 3-1B-IT (NL2KQL) & 0.821 & 0.297 & 0.074 & 0.130 & 0.055 & 2.687 & \$0.002 \\
% \midrule
% Gemma 3-4B-IT (Zero-shot) & 0.826 & 0 & 0 & 0 & 0.394 & 1.367 & \$0.001 \\
% % Gemma 3-4B-IT (Few-shot) & 0.843 & 0.513 & 0.604 & 0.424 & 0.508 & 3.351 & \$0.035 \\
% Gemma 3-4B-IT (NL2KQL) & 0.796 & 0.448 & 0.626 & 0.313 & 0.490 & 5.247 & \$0.022 \\
% \midrule
% Phi-4-Mini-Instruct (Zero-shot) & 0.570 & 0.013 & 0.043 & 0.023 & 0.385 & 1.072 & \$0.006 \\
% % Phi-4-Mini-Instruct (Few-shot) & 0.822 & 0.478 & 0.613 & 0.387 & 0.447 & 4.119 & \$0.120 \\
% Phi-4-Mini-Instruct (NL2KQL) & 0.704 & 0.296 & 0.430 & 0.300 & 0.446 & 5.375 & \$0.104 \\
% \bottomrule
% \end{tabular}
% \caption{Evaluation of multiple SLM prompting configurations, including NL2KQL, using syntax, semantic, table score, filtering accuracy, and efficiency metrics. Avg. Latency represents average amount of
% time required to complete an NLQ, and is measured in seconds per query (s/query). Total Cost (USD) represents total cost to run all 230 queries in evaluation dataset}
% \label{tab:nl2kql-slm-eval}
% \end{table*}

% \begin{table*}[!t]
% \centering
% \begin{tabular}{cccccccc}
% \toprule
% Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ & Avg. Latency & Total Cost (USD) \\
% \midrule
% Gemini 2.5 Flash & 0.939 & 0.391 & 0.526 & 0.401 & 0.548 & 7.736 & \$0.044 \\
% \midrule
% Phi-4 & 0.922 & 0.026 & 0.143 & 0.065 & 0.435 & 2.001 & \$0.010 \\
% \midrule
% Gemma-3-1B-IT & 0.504 & 0 & 0 & 0 & 0.317 & 0.825 & \$0.00 \\
% \midrule
% Gemma-3-4B-IT & 0.826 & 0 & 0 & 0 & 0.394 & 1.367 & \$0.001 \\
% \midrule
% Phi-4-Mini-Instruct & 0.570 & 0.013 & 0.043 & 0.023 & 0.385 & 1.072 & \$0.006 \\
% \bottomrule
% \end{tabular}
% \caption{RQ1: Metric results for initial LLM and SLM Prompting without Enhancements. Avg. Latency represents average amount of
% time required to complete an NLQ, and is measured in seconds per query (s/query). Total Cost (USD) represents total cost to
% run all 230 queries in evaluation dataset}
% \label{tab:llm_slm_prompting_without_enhancement}
% \end{table*}

% While the
% argument can be made that the SLMs have the capability generate syntactically correct KQL queries, they are effectively unable to produce semantically 
% correct KQL queries.

% ------


Without schema enhancements, LLMs generate syntactically correct KQL queries but struggle with semantic accuracy. OpenAI's GPT-5, GPT-4o and Google Gemini 2.0 Flash outperform Microsoft Phi-4 across all metrics, identifying relevant tables more effectively and achieving lower average latency. In contrast, SLMs perform poorly on their own: across all four tested, table score and Filter$_{\text{col}}$ remain below 0.1. While SLMs can produce syntactically valid KQL, they are largely unable to generate semantically correct queries.


This suggests that even though SLMs have the capacity to learn the structure of KQL queries, they are not independently effective in producing useful
KQL queries that can be used by security analysts. These trends are due to either SLM hallucinations of proper 
table names, hallucinations of column names, or simply due to referring to columns that do not belong within the referenced table's schema.

\input{tables/comp.tex}

\subsection{RQ2: SLMs with NL2KQL}

% \Fix{Methodology and results are blended together. There is no clear explanation of what modifications were made in the “reimplementation with substitutions” before results are presented, which can confuse readers evaluating the fairness of comparisons.}

In this research question, we determine how LLMs and SLMs perform within the NL2KQL system. We reimplement NL2KQL's architecture with some substitutions to develop a system that can be used to generate 
KQL queries, and first test this on four LLMs: OpenAI's GPT-5 and GPT-4o, Google Gemini 2.0 Flash, and Microsoft Phi-4. NL2KQL selects a certain subset of tables and their respective schema depending on the embedding of the natural 
language query, and its similarity to queries stored in a few-shot synthetic database, a database that contains realistic, example NLQ-KQL pairs. Furthermore, we test the NL2KQL configuration on four SLMs: 
Gemma-3-1B-IT, Gemma-3-4B-IT, Phi-4-Mini-Instruct, and Qwen-2.5-7B-Instruct-1M. The full prompt used in this system can be referenced within the original NL2KQL paper \cite{nl2kql}. These results are shown in Table~\ref{tab:nl2kql-llm-slm-eval}.

From the initial results shown from this table, NL2KQL when partnered with SLMs improves nearly all measured metrics when compared to the initial Zero-Shot approach. Likewise, LLMs show improvement especially 
in developing semantically correct queries when partnered with NL2KQL. Although 
SLMs can produce syntactically correct KQL queries in the Zero-Shot configuration, the model struggles to identify the correct tables to use given an NLQ. However, when NL2KQL is 
utilized to augment the SLM's knowledge, the SLMs are able to identify proper tables and columns that are needed to not only produce a syntactically correct query but a semantically correct query 
as well. However each model does incur a greater cost as opposed to utilizing the Zero-Shot approach, and also incurs more costs with respect to input and output tokens. Furthermore, the average latency per query when utilizing the NL2KQL approach 
is higher compared to the Zero-Shot approach; this is likely due to the higher number of tokens that are given to the model at inference time. We leave the NL2KQL main Avg. Latency and Total Cost 
for NL2KQL, as these values were not provided in \cite{nl2kql}.

Because the Gemma-3-4B-IT model in the NL2KQL approach performs well across all measured metrics while providing an efficient cost, we choose to build on the Gemma-3-4B-IT model while using the NL2KQL configuration
in order to increase syntactic and semantic scores of SLM-generated KQL queries.

% Old Version:
% In the following RQ, we seek to understand how adding the proper table schemas affect SLM responses for KQL queries. A schema can be defined as a list of
% columns and data types that belong to a particular table. Providing schema information to the SLM can add additional contextual information that would be useful
% in ensuring that a KQL query produced is syntactically and semantically correct. Referencing columns that correctly belong to a certain table ensures that the KQL
% query is semantically correct. To assess how well SLMs perform when given additional schema information, we utilize the Zero-Shot Prompting with Schema Context 
% prompting technique outlined in \ref{box:rq2}.

% \begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
%   colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black,
%   title=Schema Context Supplementation,fonttitle=\bfseries,
%   boxed title style={size=small,colframe=red!50!black} ]
%   You are a programmer using the Kusto Query Language with Microsoft Defender. Utilize the following tables, columns, and data types:
%   \newline
%   \newline
%   \{SCHEMA\}
%   \newline
%   \newline
%   Return only the KQL code without any explanation.
%   \newline
%   \newline
%   \{Natural Language Query\}
% \end{tcolorbox}

% We feed this prompt to the same three different SLMs: Gemma-3-1B-IT, Gemma-3-4B-IT, and Phi-4-Mini-Instruct. The results from these experiments are shown 
% in Table~\ref{tab:slm_prompting_with_enhancement}.

% \begin{table*}[!t]
% \centering
% \begin{tabular}{cccccccc}
% \toprule
% Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ & Avg. Latency & Total Cost (USD) \\
% \midrule
% Gemma-3-1B-IT & 0.635 & 0.152 & 0.074 & 0.099 & 0.203 & 2.086 & \$0.00 \\
% \midrule
% Gemma-3-4B-IT & 0.704 & 0.313 & 0.339 & 0.250 & 0.448 & 3.646 & \$0.034 \\
% \midrule
% Phi-4-Mini-Instruct & 0.604 & 0.252 & 0.452 & 0.342 & 0.384 & 3.881 & \$0.116 \\
% \bottomrule
% \end{tabular}
% \caption{RQ2: Metric results for SLM Prompting with Schema Enhancement. Avg. Latency represents average amount of time required to complete an NLQ, and is measured in seconds per query (s/query). Total Cost (USD) represents total cost to run all 230 queries in evaluation dataset}
% \label{tab:slm_prompting_with_enhancement}
% \end{table*}

% \PP{Findings}

% Across all metrics measured, there is an increase when schemas are supplied to the SLMs during prompting. However, the results do not indicate that SLMs 
% can operate independently to produce valid KQL queries. Gemma-3-4B-IT scores the highest in producing syntactically and semantically correct 
% queries across all three SLMs tested. However, Phi-4-Mini-Instruct performs better when developing queries associated with the correct table compared to Gemma-3-1B-IT 
% and Gemma-3-4B-IT. While Gemma-3-1B-IT shows some promise in developing syntactically correct queries, the model does not suffice in a zero-shot setting to produce 
% semantically correct queries even when the proper schemas are provided. However, the results across the three models indicate that SLMs have the capabilities needed
% to learn and produce a more correct KQL query. Furthermore from a pricing perspective, Phi-4-Mini-Instruct costs the most in terms of input and output tokens. Although the model fares well in terms of certain
% metrics, such as Table Score, it falls short of creating syntactically correct KQL queries in comparison to Gemma-3-1B-IT and Gemma-3-4B-IT, and incurs a higher
% average latency compared to the other two models. 

% OLD RQ3:

% In addition to supplying schema contexts, we also utilize two alternative prompting strategies: Few-Shot Prompting and NL2KQL configuration prompting. Unlike zero-shot prompting,
% few-shot prompting leverages in-context learning capabilities of both LLMs and SLMs by providing relevant examples to a problem so that the 
% model can provide a more informed response. While zero-shot prompting strategies can be effective for relatively simple tasks, few-shot prompting strategies can be 
% effectively used for more complex tasks and scenarios. 

% In the few-shot prompting scenario, we recreate the few-shot synthetic database (FSDB) proposed in NL2KQL and use this to supply the top two relevant examples into the prompt.
% As noted in NL2KQL, the number of examples stored in the FSDB is 16. To determine the most relevant examples, we use cosine similarity between the natural language query
% and the natural language queries stored in the FSDB. The initial results for different configurations are outlined in Table~\ref{tab:nl2kql-slm-eval}.

\subsection{RQ3: Different Prompt Schemes}

In this research question, we seek to understand how different prompting schemes affect NL2KQL with SLMs. In order to revise the prompt to increase the syntactic and semantic score, it is important to understand the syntactic and semantic errors that are generated
from using the Gemma-3-4B-IT and the NL2KQL configuration. For this reason, we employ Microsoft's KQL code parser \cite{Kusto-Query-Language} to first determine the types of 
syntactic errors that are generated from this scenario. Table~\ref{tab:syntax_tbl} as outlined in Appendix B outlines the list some of the most common syntactic errors returned from the KQL
queries.

From this table, one of the most common syntax errors outputted from the SLM was ”Expected ..”, composing 10.8\% of the syntax errors given. This syntax error is triggered when the KQL expression 
means to describe a timestamp between two date ranges, but does not do so correctly. For this reason, we used the Alternative Prompt \#1 strategy as 
outlined in Appendix \ref{sec:prompt-specs}. In this newer prompting strategy, the model is given proper examples of how to specify a timestamp range so
that it is able to learn from the prompt itself and not make the same mistake at inference time. Table~\ref{tab:nl2kql-eval} outlines the updated results from these tests.

% \begin{table*}[!t]
% \centering
% \begin{tabular}{cccccccc}
% \toprule
% Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ & Avg. Latency & Total Cost (USD) \\
% \midrule
% NL2KQL main & 0.988 & 0.960 & 0.822 & 0.699 & 0.666 & -- & -- \\
% \midrule
% NL2KQL + Gemma-3-4B-IT & 0.796 & 0.448 & 0.626 & 0.313 & 0.49 & 5.441 & \$0.022 \\
% \midrule
% NL2KQL + Gemma-3-4B-IT (Revised) & 0.861 & 0.578 & 0.643 & 0.394 & 0.486 & 1.961 & \$0.009 \\
% \bottomrule
% \end{tabular}
% \caption{Metric results and comparison between NL2KQL, original prompting strategy, and the alternative rompting strategy for Gemma-3-4B-IT}
% \label{tab:revised_prompting}
% \end{table*}

% (s/query)
% OTHER STATS: Average Latency - 3.297094857174417, Total Tokens: 1724018

With the revised prompting strategy, there is an increase in the syntax, semantic, and Filter$_{\text{col}}$ scores of KQL queries. After calculating
these metrics, we also analyze the remaining types of syntax errors that exist after the prompting alteration. These results are outlined in Table~\ref{tab:syntax_tbl_additional}. These results indicates that the number of syntax errors associated decreased, and the number of errors with the message: "Expected .." decreased 
as well. After taking this approach, we begin looking at alternative prompts that might be useful to improve the semantic score. In order to further improve the semantic score, 
it is imperative to understand the components where the model is unable to generate correct KQL queries. We utilize the same KQL code parser to further analyze the semantic errors that are returned within the KQL code. Table~\ref{tab:semantic-tbl} outlines the list of the most common syntax errors returned from the KQL queries. From this table, some of the semantic errors that are given from the model involve improperly referenced columns. This could be due to either SLM hallucinations or due to referencing columns that do not belong to a certain table's schema. In either
case, we seek to improve the semantic score by employing Alternative Prompt \#2 as outlined in Appendix \ref{sec:prompt-specs}. These results are shown in Table~\ref{tab:nl2kql-eval}.

\input{tables/semantic.tex}

Despite specifying that multiple rules to assist the SLM in producing semantically correct queries, the SLM performance across all metrics measured
does not appear to improve with this revised prompting technique. Compared to the first revised prompting technique, the Gemma-3-4B-IT model performs 
considerably worse across all metrics. This shows that even when given a proper table schema, SLMs cannot always verify independently whether KQL queries 
are semantically correct. Furthermore, Table~\ref{tab:semantic-tbl-additional} outlines that columns and other variables remain undefined even after specifying 
the SLM to follow the correct schemas.

\subsection{RQ4: LoRA Fine Tuning}

% \Fix{Don't use the word multi-SLM architecture.}

% \Fix{why discuss applying the best fine-tuned model configuration to the multi-SLM architecture before presenting RQ4 results. Don't discuss Two-Stage architecture in this RQ.}

In this research question, we also seek to understand how LoRA Fine-Tuning performs on the Gemma-3-4B-IT SLM. Because the Gemma-3-4B-IT SLM has shown improved promise in generating KQL 
queries with the help of revised prompting techniques, a full-fine tuning of model parameter weights may be excessive. As an alternative, LoRA Fine-Tuning provides an efficient method of fine-tuning a model by minimizing computational 
costs needed to alter model weights. We first perform a hyperparameter search that minimizes the cross-entropy validation set loss of the Google Gemma-3-4B-IT SLM. Once we have found the set of parameters 
that minimizes the validation set loss, we test this configuration on the NL2KQL system. As shown in Table~\ref{tab:alpharank}, the SLM configuration when $\alpha=8$, $r=4$, and LoRA dropout = 0.2 produced 
an validation set loss of 0.667. Because this configuration minimized the evaluation loss, we use the fine-tuned model from this configuration to insert into the NL2KQL system. The results from this configuration
are shown in Table~\ref{tab:nl2kql-eval} under Advanced Configurations.

% \Fix{Make sure you put Table in Appendix!}

% Table (~reference in Appendix) outlines both training loss and evaluation loss for each model configuration tested. Of the configurations tested, the SLM configuration when $\alpha=8$, $r=4$, and LoRA dropout = 0.2 produced 
% an evaluation loss of 0.027. Because this configuration minimized the evaluation loss, we use the fine-tuned model from this configuration to insert into the two-staged architecture. The results from the two-staged architecture that leverages these 
% model configurations is shown in Table~\ref{tab:nl2kql-eval}. 

The fine-tuned model performs worse in producing syntactically and semantically correct KQL queries compared to the original prompting techniques. However, the LoRA Fine-Tuned versions of Gemma-3-4B-IT, when utilized in the NL2KQL system, are able to perform slightly 
better in generating semantically correct KQL queries. Furthermore, the costs of running the fine-tuned model, assuming the same input/output token costs 
as the Gemma-3-4B-IT, remain roughly the same. 

It is possible that due to the nature of reasoning SLMs, supervised fine-tuning alone may not be sufficient in order to train SLMs. When fine-tuning SLMs to improve phishing email detection, researchers 
found that directly fine-tuning models small reasoning models such as Llama-3.2-3B-Instruct and Qwen-2.5-1.5B-Instruct on phishing emails and respective labels yielded poor results when compared to vanilla prompting strategies \cite{Improving_Phishing_Email_Detection}. For this reason, we also 
attempt a COT approach, and augment each NLQ--KQL pair with a short explanation generated by the teacher (Gemini~2.0 Flash) of how the KQL result was generated. This provides the SLM model the proper reasoning behind a KQL query result in addition to the actual KQL answer. As outlined in Table~\ref{tab:alpharank_two},
since $\alpha=8$, $r=4$, and LoRA dropout = 0.1 minimized validation set loss to 0.668, we use the fine-tuned model with these parameters. These results are outlined in Table~\ref{tab:nl2kql-eval} under Advanced Configurations. The COT Fine-Tuning approach does much better than the supervised fine-tuning approach 
in producing syntactically and semantically correct KQL queries. However, it does not surpass the different prompting strategies that have been attempted.

\subsection{RQ5: Two-Staged Architecture}

% \Fix{The subsection claims Gemini costs which directly contradicts the Cost Analysis table earlier.}

% \Fix{“This proposal draws upon Langchain in Python to query a single instance of Gemma-3-4B-IT, collect the result once they have been outputted by the SLM” is grammatically awkward.}

% \Fix{The subsection blends architectural description, prompting strategies, and final performance outcomes in a single narrative. There is no clear separation between what was proposed, how it was evaluated, and what the results were.}

Building on the evaluation from \textbf{RQ3}, we propose a two-Staged Architecture that combines components of the NL2KQL architecture, leverages an SLMs instance of Google Gemma-3-4B-IT, 
and uses an oracle LLM, Google Gemini 2.0 Flash, to verify that KQL queries are syntactically and semantically correct. We introduce the SLMs instance and set its temperature equal to 1. While SLMs can perform well at producing syntactically correct queries, leveraging an oracle LLM model to process the outputs can ensure that KQL queries that are produced are refined to produce more semantically correct 
queries. We choose Google Gemini 2.0 Flash due to its relatively low cost for input tokens and output tokens while also yielding syntactically and semantically correct KQL queries. 

This solution queries Gemma-3-4B-IT, collects the result once they have been outputted by the SLM,
and allow an Oracle model to refine the best query as much as possible. To test the efficacy of the two-staged architecture as a whole, we leveraged different prompting
techniques for the Oracle model, as outlined in \ref{fig:oracle-prompting}. This allows us to understand how useful an oracle refinement is with respect to the SLM that generate KQL queries. In the first oracle prompting technique, 
the oracle model chooses the best response and regenerates a proper KQL query if the best response is not syntactically or semantically correct according to the Oracle model itself. In the second oracle prompting technique, 
the oracle model chooses the best response and regenerates a proper KQL query with additional schema context provided from the Schema Refiner in the NL2KQL architecture.
Furthermore, we test the NL2KQL configuration with respect to the best prompting strategy and assess how the system performs with respect to the Oracle methods as well.
The results from these varying Oracle prompts is shown in Table~\ref{tab:nl2kql-eval}.


From the results noted, the revised prompting strategy performs better than the original NL2KQL prompting strategy with respect to generating 
syntactically and semantically correct queries. Furthermore, of the two solution strategies, the two-staged architecture solution with 
schema context performs the best in terms of generating syntactic and semantically correct KQL queries. These metrics beat the NL2KQL Configuration that uses 
Gemini 2.0 Flash, while costing over ten times cheaper to run through the entire evaluation set. However, it does not beat the NL2KQL (reported) results in terms of syntax 
and semantic score. When the two-staged architecture solution is told to simply pick the best solution 
provided by the LLMs, the two-staged architecture solution produces slightly better syntactically correct queries than NL2KQL and the revised prompting solution. However, when we 
allow the oracle model to refine the KQL queries generated by the SLMs, the two-staged architecture solution produces near perfect syntactically correct queries but mediocre semantically 
correct queries. 




% \midrule
% NL2KQL + Multi-Agent Solution (Retrieval Only) & 0.852 & 0.604 & 0.530 & 0.455 & 0.476 & -- & -- \\
% \midrule
% NL2KQL + Multi-Agent Solution (Retrieval + General Refinement) & 0.957 & 0.583 & 0.552 & 0.461 & 0.542 & -- & -- \\
% \midrule
% NL2KQL + Multi-Agent Solution (Retrieval + Schema Refinement) & 0.965 & 0.848 & 0.639 & 0.541 & 0.568 & -- & -- \\

% \begin{tcolorbox}[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm},
%   colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black,
%   title=Alternative Prompt,fonttitle=\bfseries,
%   boxed title style={size=small,colframe=red!50!black} ]
%   You are a programmer using the Kusto Query Language with Microsoft Defender. Use the following tables, columns and data types. You may use more than one table for the KQL query, but you are not required to use all Columns and Data Types:
%   \newline
%   \newline
%   \{SCHEMA\}
%   \newline
%   \newline
%   Return only the KQL code without any explanation.
%   \newline
%   \newline
%   \{Natural Language Query\}
% \end{tcolorbox}

% In the prompt above, we ask the model to perform the same task in a zero-shot setting. However, as an addditional prompting component, we instruct
% the model that it does not need to utilize all of the columns provided in the schema. The main purpose behind this is to attempt and improve results 
% by only utilizing table and columns that are necessary to build a proper KQL query. Because the Filter$_{\text{lit}}$ scores are negatively penalized
% for referencing more columns than necessary to build a KQL query, this provides a way for the SLM to reason about what columns are really needed for
% reference. The results from the revised prompt as well as a comparison to NL2KQL and the previous prompting methodology are shown in Table III.

% \begin{table}[H]
%   \centering
%   \footnotesize, \scriptsize, \tiny
%   \setlength{\tabcolsep}{0.57\tabcolsep}
%   \begin{tabular}{cccccc}
%     \toprule
%     Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ \\
%     \midrule
%     NL2KQL & 0.988 & 0.960 & 0.822 & 0.699 & 0.666 \\
%     \midrule
%     Gemma-3-4B-IT (Original) & 0.704 & 0.313 & 0.339 & 0.250 & 0.448 \\
%     \midrule
%     Gemma-3-4B-IT (Revised) & \textbf{0.795} & \textbf{0.613} & \textbf{0.865} & \textbf{0.569} & \textbf{0.518} \\
%     \bottomrule
%   \end{tabular}
%   \caption{Metric results and comparison between NL2KQL, original prompting strategy, and revised prompting strategy for Gemma-3-4B-IT}
% \end{table}

% With the revised prompting strategy, there is an increase across all metrics defined. Tthere is a significant increase in the Filter Literal score, 
% which indicates that the model is able to utilize only the tables and columns needed to construct the KQL query when instructed only to use the columns
% as needed. After taking this approach, we begin looking at improving the syntax score. In order to further improve the syntax score, it is imperative to 
% understand the components where the model is unable to generate correct KQL queries. For this reason, we utilize a KQL code parser developed by 
% Microsoft to analyze the syntax errors that are returned within the KQL code. Table IV outlines the list of the most common syntax errors returned 
% from the KQL queries (NEEDS REVISION):

% \begin{table}[H]
%   \centering
%   \begin{tabular}{cc}
%     \toprule
%     \textbf{Syntax Error} & \textbf{Count} \\
%     \midrule
%     Missing Expression & 27 \\
%     \midrule
%     Expected .. & 26 \\
%     \midrule
%     Expected ) & 15 \\
%     \midrule
%     Expected ; & 15 \\
%     \bottomrule
%   \end{tabular}
%   \caption{List of most common syntax errors associated with the revised prompting strategy.}
% \end{table}

% From this table, one of the most common syntax errors outputted from the SLM was "Expected ..". This syntax error is triggered when the KQL expression
% means to describe a timestamp between two date ranges, but does not do so correctly. To rectify this problem, we attempt a different prompting strategy 
% as follows:

% % \begin{framed}
% % \begin{spverbatim}
% % <start_of_turn>
% % Convert the following natural language query to Structured Query Language (SQL). Then, convert the SQL query to Kusto Query Language (KQL). Focus on producing syntactically correct KQL queries, you may use the following tips to help: 

% % Here are some examples of how to describe a timestamp:  between(datetime("2025-01-01T12:00:00Z") .. datetime("2025-01-01T12:01:00Z")), Timestamp < ago(7d)

% % Use the following tables, columns and data types. If more than one table is listed, use all tables mentioned. You are not required to use all Columns and Data Types:

% % Tables: 
% % Columns and Data Types: 
% % Natural Language Query: 
% % <end_of_turn>
% % <start_of_turn>model
% % \end{spverbatim}
% % \end{framed}

% In this newer prompting strategy, the model is given proper examples of how to specify a timestamp range so that it is able to learn from the prompt itself
% and not make the same mistake at inference time. Table V outlines the updated results from these tests:

% \begin{table}[H]
%   \centering
%   \footnotesize, \scriptsize, \tiny
%   \setlength{\tabcolsep}{0.2\tabcolsep}
%   \begin{tabular}{cccccc}
%     \toprule
%     Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ \\
%     \midrule
%     NL2KQL & 0.988 & 0.960 & 0.822 & 0.699 & 0.666 \\
%     \midrule
%     Gemma-3-4B-IT (Original) & 0.770 & 0.587 & 0.852 & 0.409 & 0.503 \\
%     \midrule
%     Gemma-3-4B-IT (Revised) & 0.795 & 0.613 & \textbf{0.865} & \textbf{0.569} & \textbf{0.518} \\
%     \midrule
%     Gemma-3-4B-IT (Revised \#2) & \textbf{0.865} & \textbf{0.651} & 0.822 & 0.550 & 0.493 \\
%     \bottomrule
%   \end{tabular}
%   \caption{Metric results and comparison between NL2KQL, first revised prompting strategy, and newer prompting strategy for Gemma-3-4B-IT}
% \end{table}

% With the new prompting strategy, the syntax and semantic scores greatly improve. In addition to improving the syntax score, we also aim to improve
% the semantic score as well. Although SLM-generated KQL queries may be syntactically correct, it does not necessarily mean that they are correct with
% respect to a table's schema. For this reason, we also gather the most common semantic errors that exist as a result of using the same prompt. The most
% common semantic errors are highlighted in Table VI.

% \begin{table}[H]
%   \centering
%   \setlength{\tabcolsep}{0.2\tabcolsep}
%   \begin{tabular}{cc}
%     \toprule
%     \textbf{Semantic Error} & \textbf{Count} \\
%     \midrule
%     Expected ; & 31 \\
%     \midrule
%     The incomplete fragment is unexpected. & 15 \\
%     \midrule
%     The expression must have the type bool. & 8 \\
%     \midrule
%     Timestamp Error & 8 \\
%     \midrule
%     Contains Error & 5 \\
%     \bottomrule
%   \end{tabular}
%   \caption{List of most common semantic errors associated with the revised prompting strategy.}
% \end{table}

% As highlighted in the table above, there are many key semantic errors that contribute to a lower syntax and semantic score.
% For this reason, we test a third prompt with the Gemma-3-4B model as follows:

% % \begin{framed}
% % \begin{spverbatim}
% % <start_of_turn>user
% % Write a code snippet in Kusto Query Language (KQL) for the following Natural Language Query. Let's think step by step:\n\n

% % 1. First write a Structured Query Language (SQL) query.
% % 2. Convert this SQL query to KQL.

% % Focus on producing syntactically and semantically correct KQL queries, you may use the following tips to help as well:

% % 1. Use the following structures to describe a timestamp: between(datetime("2025-01-01T12:00:00Z") .. datetime("2025-01-01T12:01:00Z")), Timestamp < ago(7d)
% % 2. Use the `has_any` or `has_all` keywords when searching for multiple strings in a column. Use the following structure: "Column has_all ("A", "B", "C")", "Column has_any("A", "B", "C")" instead of "Column contains("A", "B", "C")".
% % 3. Instead of using "project *" to return all columns when generating a KQL query, omit this statement entirely.
% % 4. Instead of using the structure "X greater than ago(7d)", use "X > ago(7d)"
% % 5. Instead of using "InitiatingProcessName", use "InitiatingProcessFileName".
% % 6. Use only the following tables, columns and data types provided. If more than one table is listed, use all tables mentioned. You are not required to use all Columns and Data Types, but only use the columns mentioned:

% % Tables:
% % Columns and Data Types:
% % Natural Language Query:
% % <end_of_turn><start_of_turn>model
% % \end{spverbatim}
% % \end{framed}

% The results from this prompting techniques are shown in the table below:

% \begin{table}[H]
%   \centering
%   \footnotesize, \scriptsize, \tiny
%   \setlength{\tabcolsep}{0.2\tabcolsep}
%   \begin{tabular}{cccccc}
%     \toprule
%     Model & Syntax & Semantic & Table Score & Filter$_{\text{col}}$ & Filter$_{\text{lit}}$ \\
%     \midrule
%     NL2KQL & 0.988 & 0.960 & 0.822 & 0.699 & 0.666 \\
%     \midrule
%     Gemma-3-4B-IT (Original) & 0.770 & 0.587 & 0.852 & 0.409 & 0.503 \\
%     \midrule
%     Gemma-3-4B-IT (Revised) & 0.795 & 0.613 & 0.865 & \textbf{0.569} & \textbf{0.518} \\
%     \midrule
%     Gemma-3-4B-IT (Revised \#2) & 0.865 & 0.651 & 0.822 & 0.550 & 0.493 \\
%     \midrule
%     Gemma-3-4B-IT (Revised \#3) & \textbf{0.891} & \textbf{0.735} & \textbf{0.857} & 0.506 & 0.480 \\
%     \bottomrule
%   \end{tabular}
%   \caption{Metric results and comparison between NL2KQL, first revised prompting strategy, and newer prompting strategy for Gemma-3-4B-IT}
% \end{table}



