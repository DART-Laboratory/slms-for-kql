\section{Related Work}
\label{s:relwk}

% \Fix{Create three highlevel domain. one could be Security-oriented LLMs. There is a growing body of work
% on developing security-specific LLMs blah blah. }

% \Fix{Think of other two high level domains and put the following works into that. Remove any redudant information. Look at the papers you read and see how the structure related work section.}

% Consider the following breakdown: Security-Specific LLMs, LLMs for Code Generation, Code Generation for Domain-Specific Languages

\heading{Natural Language to Query Generation} Converting NLQs to structured queries has been widely studied across domains and languages. Early work \cite{popescu2003towards,li2014nalir,baik2019bridging} translated NLQs to SQL using rule-based methods and custom heuristics, but extending these techniques to new databases and languages demands substantial manual effort and struggles with edge cases \cite{nl2kql}. Neural methods improve portability by casting text-to-SQL as sequence generation with integrated validation. \textit{PICARD} \cite{PICARD} uses sequence-to-sequence models with modular syntactic checks to ensure query correctness and alignment with the NLQ. Supplying schema context further improves semantic fidelity \cite{Hong_Yuan_Chen_Zhang_Huang_Huang_2024}. In practice, these ideas combine constrained decoding for syntax with schema-aware prompting or retrieval for semantics, directly informing our KQL setting. Unlike prior systems that center on a single model, our approach uses both SLMs and LLMs. These general principles have been extended to domain-specific query languages, especially in security, where KQL is the primary language for threat hunting and log analysis. \textit{NL2KQL} \cite{nl2kql} translates NLQs to KQL by combining embedding-based semantic similarity with few-shot prompting in a modular, end-to-end framework. \textit{XPert} \cite{X_Pert} takes an incident-centric approach, retrieving similar historical tickets to recommend or compose queries using in-context learning with embedding-based retrieval. However, both systems rely heavily on LLMs, resulting in cost and latency constraints that limit real-time SOC deployment, where analysts need rapid query assistance. In contrast, our approach uses an SLM for generation, avoiding exclusive dependence on LLMs.

% \Fix{Add a sentence that contrast all these system with our system and say why our system is better or different from all these system.}

\heading{Security-Specific LLMs}
LLMs are increasingly applied across SOC tasks and offensive evaluations, but their effectiveness hinges on task-specific scaffolding and verification. \textit{VulDetect} \cite{VulDetect} fine-tunes GPT-style models for anomaly detection in logs using a transformer-based framework. \textit{PentestGPT} \cite{PentestGPT} builds an LLM penetration-testing assistant with modular task support, reporting bounded gains on real targets, highlighting the importance of structured tooling around the model. Although LLMs offer strong code generation capabilities, their cost and latency often make real-time deployment impractical~\cite{Belcak_Heinrich_Diao_Fu_Dong_Muralidharan_Lin_Molchanov}, pushing real-time operations toward SLMs.

\heading{SLMs Adoption}
SLMs provide attractive latency and cost profiles for SOC settings but require augmentation to match LLM quality. SuperICL shows that small, locally fine-tuned models can act as plug-ins to larger LLMs, structuring context while the larger model executesâ€”a useful division of labor for SOC workflows \cite{SLM_Ref}. Parameter-efficient fine-tuning further narrows this gap while preserving efficiency; methods like LoRA and QLoRA enable targeted adaptation on modest hardware \cite{LoRA,Improving_Phishing_Email_Detection,qlora}. Complementary techniques such as schema/value retrieval, few-shot selection, and best-of-N with an external judge improve correctness without enlarging model footprint \cite{wang2022self,madaan2023self}. In sum, SLMs are viable for SOC use when paired with retrieval, schema hints, and parameter-efficient tuning.

% KQL generation framework, including a Semantic Data Catalog, Schema Refiner, Few-Shot Database, and a Query Refiner. In addition to 
% providing a comprehensive framework, this project also releases an evaluation benchmark dataset that can be utilized for KQL query 
% generation correctness. This benchmark consists of over 400 NLQ-KQL pairs that have been compiled with the help of KQL experts and 
% distributed across two Kusto Databases: Microsoft Defender and Microsoft Sentinel. Microsoft Defender is a unified pre- and 
% post-breach enterprise defense suite that natively coordinates detection, prevention, investigation, and response across endpoints, 
% identities, email, and applications to provide integrated protection against sophisticated attacks \cite{Defender_Info}. Microsoft 
% Sentinel is a scalable, cloud-native SIEM that delivers an intelligent and comprehensive solution for SIEM and security orchestration, 
% automation, and response (SOAR) \cite{Sentinel_Info}. These NLQ-KQL pairs serve as the main evaluation data for our proposed solution. 
% Although NL2KQL has been developed in an effort to assist in converting NLQs to KQL queries, however we also highlight some painpoints 
% that may exist within the pipeline. 

% The current end-to-end system leverages a synthetic few-shot database in efforts to guide the LLM by providing similar cyber incident 
% scenarios and their respective KQL ground truth queries (i.e. NLQ-KQL pair). However, the few-shot database that contains these KQL queries
% consists of KQL queries that have been randomly generated with the help of LLMs, and there is little mention of human-review for these KQL queries.
% This calls into question whether the KQL queries generated with the help of the LLM resemble real-life scenarios that mimic those faced by security 
% analysts on a daily basis. Furthermore, external open-source resources contain NLQ-KQL pairs whose NLQ structure differs greatly from the NLQ 
% generated by LLMs. Another crucial component in the NL2KQL pipeline is the Schema Refiner, as it determines the relevant tables and columns given 
% an NLQ from the user. The Schema Refiner is based on the similarity of Natural Language Queries (NLQ) embeddings with the Data Catalog Elements 
% We argue that current methods utilized in the Schema Refiner do not appear sufficient in improving table and column accuracies.

% \subsection{XPert}
% XPert is an end-to-end Machine Learning framework that is also designed to automate the KQL recommendation process. XPert builds 
% on historical incident ticket data to either recommend past KQL queries that have been found in past tickets, or generate new 
% KQL queries for newer incidents. The solution's architecture is composed of multiple modules, including an Incident Data Processor, 
% an Incident Retrieval Vector Database that is leveraged for prompt construction, and a Post-Processor. The Incident Data Processor 
% stores rich incident data of past tickets found within incident management systems, including metadata, summary, and discussion of 
% previous tickets. XPert then leverages in-context learning capabilities from LLMs to retrieve incidents that are similar to
% the corresponding queries. These similarities are calculated with the help of an embedding model for historical incidents
% and the target incident.

% XPert also introduces multiple metric assessments, many of which are leveraged used for the purposes of this paper. In order
% to assess the executability of the KQL query, XPert uses a syntax-checker to assess syntactical errors in the LLM-generated query
% and a static analysis approach to determine the semantic correctness of a query. Furthermore, XPert introduces the notion of
% Sub-Component Matching. Sub-Component Matching is meant to assess the lexical similarity between LLM-generated queries and their
% respective ground truths. This is done by computing the F1 score between the sets of operands in both queries. For the purposes of
% this project, we make use of Sub-Component Analysis to understand whether LLM-generated queries can generate similar filters to
% their respective ground truth KQL queries.

